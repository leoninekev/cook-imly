{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps #####\n",
    "\n",
    "1. Detect and Load dataset\n",
    "2. Keras\n",
    "3. Scikit\n",
    "4. Kfold\n",
    "5. ROC curve\n",
    "6. Classification report\n",
    "7. Write back the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detecting and loading data #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('../data/client_secret.json', scope)\n",
    "\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "sh = gc.open('Dataset details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet = sh.get_worksheet(0)\n",
    "dataset_list = worksheet.col_values(2)\n",
    "params_list = worksheet.row_values(2)[7:]\n",
    "params_dict = {x:i+7 for i,x in enumerate(params_list)}\n",
    "activation_list = worksheet.col_values(3)\n",
    "dataset_list\n",
    "\n",
    "# for data in dataset_list:\n",
    "#     if data follows a specific format then:\n",
    "#         process the dataset\n",
    "#         Create a list of dataframes\n",
    "row_nb = 3\n",
    "n_col_nb = 4 \n",
    "p_col_nb = 5\n",
    "c_col_nb = 7\n",
    "activation_function = activation_list[row_nb]\n",
    "data_url = dataset_list[row_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep for iris dataset #\n",
    "\n",
    "data = pd.read_csv(data_url, delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "n=data.shape[0]\n",
    "p=X.shape[1]\n",
    "\n",
    "unique,count = np.unique(Y,return_counts=True)\n",
    "class_distribution = str(count[0]) + \" : \" + str(count[1])\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'76.0 : 24.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data prep for Adult salary dataset #\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "data = pd.read_csv(data_url, delimiter=\",\", header=None, index_col=False,names=names)\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "\n",
    "# data.iloc[:,-1] = index\n",
    "# data = data.loc[data[4] != 2]\n",
    "# X = data.iloc[:,:-1]\n",
    "# Y = data.iloc[:,-1]\n",
    "n=data.shape[0]\n",
    "p=X.shape[1]\n",
    "\n",
    "unique,count = np.unique(Y,return_counts=True)\n",
    "class1=count[0]/data.shape[0]*100\n",
    "class2=count[1]/data.shape[0]*100\n",
    "class_distribution = str(round(class1)) + \" : \" + str(round(class2))\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Class1_distribution=Y[1].value_counts()[0] / Y.shape[0] * 100\n",
    "# Class2_distribution=Y[1].value_counts()[1] / Y.shape[0] * 100\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras model #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19537/19537 [==============================] - 1s 36us/step\n",
      "\n",
      "acc: 75.66%\n"
     ]
    }
   ],
   "source": [
    "### Logistic regression using DNN ###\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# Defining model #\n",
    "if activation_function == \"Logistic regression\":\n",
    "    activation = \"sigmoid\"\n",
    "input_dim = X.shape[1]\n",
    "epoch = 2000 # Add in the sheet\n",
    "batch_size = 300 # Add in the sheet\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1,input_dim=input_dim,activation=activation))\n",
    "\n",
    "# Compile the model #\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model #\n",
    "\n",
    "# model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=10)\n",
    "model.fit(x_train, y_train, epochs=epoch, batch_size=batch_size, verbose=0)\n",
    "# Evaluate the model #\n",
    "\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model details to be added**\n",
    "1. Score/Accuracy\n",
    "2. bias_initializer\n",
    "3. kernel_regularizer\n",
    "4. bias_regularizer\n",
    "5. layer name\n",
    "6. use bias\n",
    "7. kernel initializer\n",
    "8. activity regularizer\n",
    "9. kernel constraint\n",
    "10. bias constraint\n",
    "\n",
    "*What do these values represent?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'dense_3',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None, 14),\n",
       " 'dtype': 'float32',\n",
       " 'units': 1,\n",
       " 'activation': 'sigmoid',\n",
       " 'use_bias': True,\n",
       " 'kernel_initializer': 'VarianceScaling',\n",
       " 'bias_initializer': 'Zeros',\n",
       " 'kernel_regularizer': None,\n",
       " 'bias_regularizer': None,\n",
       " 'activity_regularizer': None,\n",
       " 'kernel_constraint': None,\n",
       " 'bias_constraint': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_params = model.get_config()\n",
    "keras_params = keras_params['layers'][0]['config']\n",
    "keras_params['kernel_initializer'] = keras_params['kernel_initializer']['class_name']\n",
    "keras_params['bias_initializer'] = keras_params['bias_initializer']['class_name']\n",
    "keras_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit model #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7898346726723653\n"
     ]
    }
   ],
   "source": [
    "# Scikit learn #\n",
    "\n",
    "# Import and create an instance of your model(Logistic regression)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "# Train your model using the training dataset\n",
    "\n",
    "logisticRegr.fit(x_train,y_train)\n",
    "\n",
    "# Predict the output \n",
    "\n",
    "predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "score = logisticRegr.score(x_test,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model details to collect #####\n",
    "1. All values returned by get_params method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit_params = logisticRegr.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing the values back to the sheet #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'ovr',\n",
       " 'n_jobs': 1,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scikit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1E5jcq2w42gN8bMIaeaRJpAdhgSVN-2XDJ_YTHe4qfwY',\n",
       " 'updatedRange': 'Sheet1!G4',\n",
       " 'updatedRows': 1,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedCells': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steps #\n",
    "# 1. Read and figure out the relevant column and row\n",
    "# 2. Map the Keras/Scikit dicts with the columns and write to the corresponding cells\n",
    "# keras_params['layers']\n",
    "# scikit_params\n",
    "for param,col_nb in params_dict.items():\n",
    "    for s_param,value in scikit_params.items():\n",
    "        if param == s_param:\n",
    "            if value == None:\n",
    "                value = 'None'\n",
    "            worksheet.update_cell(row_nb+1, col_nb+1, value)\n",
    "            \n",
    "\n",
    "for param,col_nb in params_dict.items():\n",
    "    for k_param,value in keras_params.items():\n",
    "        if param == k_param:\n",
    "            if value == None:\n",
    "                value = 'None'\n",
    "            worksheet.update_cell(row_nb+1, col_nb+1, value)\n",
    "worksheet.update_cell(row_nb+1, n_col_nb, n)\n",
    "worksheet.update_cell(row_nb+1, p_col_nb, p)\n",
    "worksheet.update_cell(row_nb+1, c_col_nb, class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
