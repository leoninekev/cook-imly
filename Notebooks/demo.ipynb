{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do -  \n",
    "1) Monkey patching sklearn models\n",
    "    + Do it for one method - say 'fit'\n",
    "    + Replicate this for the remaining methods\n",
    "2) Mapping -- tineyDB part  \n",
    "3) Testing -- writing learnings from the experiment  \n",
    "to the master sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo for Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical representation\n",
    "1) Fit  \n",
    "2) Predict  \n",
    "3) Score  \n",
    "4) Loss  \n",
    "5) Optimization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $ Y_i = \\beta_0 + \\beta_1 x_{i1} +\\cdots + \\beta_p x_{ip} + \\epsilon_i,\\qquad i = 1,\\ldots,n $  \n",
    "\n",
    "+ **Score**\n",
    "  + $ {MSE} = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\hat Y_i)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from imly import dope\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "sc = StandardScaler()\n",
    "diabetes.data = sc.fit_transform(diabetes.data)\n",
    "\n",
    "x = diabetes_X\n",
    "y = diabetes.target\n",
    "\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "x_train = diabetes_X[:-20]\n",
    "x_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = diabetes.target[:-20]\n",
    "y_test = diabetes.target[-20:]\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "m = dope(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(diabetes.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [11:31<00:00,  7.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n"
     ]
    }
   ],
   "source": [
    "m.fit(x_train,y_train) # executes keras fit by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = m.predict(x_test) # executes keras predict_class method by default\n",
    "\n",
    "# score = m.evaluate(x_test, y_test) # dnn equivalent\n",
    "\n",
    "onnx_model = m.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(x_train,y_train, using = 'sklearn') # executes sklearn fit\n",
    "\n",
    "y_pred = m.predict(x_test, using='sklearn') # executes sklearn predict\n",
    "\n",
    "score = m.score(x_test, y_test, using='sklearn') # sklearn equivalent\n",
    "\n",
    "m.save(using = 'sklearn') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def fit(self,\n",
      "            x=None,\n",
      "            y=None,\n",
      "            batch_size=None,\n",
      "            epochs=1,\n",
      "            verbose=1,\n",
      "            callbacks=None,\n",
      "            validation_split=0.,\n",
      "            validation_data=None,\n",
      "            shuffle=True,\n",
      "            class_weight=None,\n",
      "            sample_weight=None,\n",
      "            initial_epoch=0,\n",
      "            steps_per_epoch=None,\n",
      "            validation_steps=None,\n",
      "            **kwargs):\n",
      "        \"\"\"Trains the model for a given number of epochs (iterations on a dataset).\n",
      "\n",
      "        # Arguments\n",
      "            x: Numpy array of training data (if the model has a single input),\n",
      "                or list of Numpy arrays (if the model has multiple inputs).\n",
      "                If input layers in the model are named, you can also pass a\n",
      "                dictionary mapping input names to Numpy arrays.\n",
      "                `x` can be `None` (default) if feeding from\n",
      "                framework-native tensors (e.g. TensorFlow data tensors).\n",
      "            y: Numpy array of target (label) data\n",
      "                (if the model has a single output),\n",
      "                or list of Numpy arrays (if the model has multiple outputs).\n",
      "                If output layers in the model are named, you can also pass a\n",
      "                dictionary mapping output names to Numpy arrays.\n",
      "                `y` can be `None` (default) if feeding from\n",
      "                framework-native tensors (e.g. TensorFlow data tensors).\n",
      "            batch_size: Integer or `None`.\n",
      "                Number of samples per gradient update.\n",
      "                If unspecified, `batch_size` will default to 32.\n",
      "            epochs: Integer. Number of epochs to train the model.\n",
      "                An epoch is an iteration over the entire `x` and `y`\n",
      "                data provided.\n",
      "                Note that in conjunction with `initial_epoch`,\n",
      "                `epochs` is to be understood as \"final epoch\".\n",
      "                The model is not trained for a number of iterations\n",
      "                given by `epochs`, but merely until the epoch\n",
      "                of index `epochs` is reached.\n",
      "            verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      "                0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            callbacks: List of `keras.callbacks.Callback` instances.\n",
      "                List of callbacks to apply during training.\n",
      "                See [callbacks](/callbacks).\n",
      "            validation_split: Float between 0 and 1.\n",
      "                Fraction of the training data to be used as validation data.\n",
      "                The model will set apart this fraction of the training data,\n",
      "                will not train on it, and will evaluate\n",
      "                the loss and any model metrics\n",
      "                on this data at the end of each epoch.\n",
      "                The validation data is selected from the last samples\n",
      "                in the `x` and `y` data provided, before shuffling.\n",
      "            validation_data: tuple `(x_val, y_val)` or tuple\n",
      "                `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      "                the loss and any model metrics at the end of each epoch.\n",
      "                The model will not be trained on this data.\n",
      "                `validation_data` will override `validation_split`.\n",
      "            shuffle: Boolean (whether to shuffle the training data\n",
      "                before each epoch) or str (for 'batch').\n",
      "                'batch' is a special option for dealing with the\n",
      "                limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "                Has no effect when `steps_per_epoch` is not `None`.\n",
      "            class_weight: Optional dictionary mapping class indices (integers)\n",
      "                to a weight (float) value, used for weighting the loss function\n",
      "                (during training only).\n",
      "                This can be useful to tell the model to\n",
      "                \"pay more attention\" to samples from\n",
      "                an under-represented class.\n",
      "            sample_weight: Optional Numpy array of weights for\n",
      "                the training samples, used for weighting the loss function\n",
      "                (during training only). You can either pass a flat (1D)\n",
      "                Numpy array with the same length as the input samples\n",
      "                (1:1 mapping between weights and samples),\n",
      "                or in the case of temporal data,\n",
      "                you can pass a 2D array with shape\n",
      "                `(samples, sequence_length)`,\n",
      "                to apply a different weight to every timestep of every sample.\n",
      "                In this case you should make sure to specify\n",
      "                `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      "            initial_epoch: Integer.\n",
      "                Epoch at which to start training\n",
      "                (useful for resuming a previous training run).\n",
      "            steps_per_epoch: Integer or `None`.\n",
      "                Total number of steps (batches of samples)\n",
      "                before declaring one epoch finished and starting the\n",
      "                next epoch. When training with input tensors such as\n",
      "                TensorFlow data tensors, the default `None` is equal to\n",
      "                the number of samples in your dataset divided by\n",
      "                the batch size, or 1 if that cannot be determined.\n",
      "            validation_steps: Only relevant if `steps_per_epoch`\n",
      "                is specified. Total number of steps (batches of samples)\n",
      "                to validate before stopping.\n",
      "\n",
      "        # Returns\n",
      "            A `History` object. Its `History.history` attribute is\n",
      "            a record of training loss values and metrics values\n",
      "            at successive epochs, as well as validation loss values\n",
      "            and validation metrics values (if applicable).\n",
      "\n",
      "        # Raises\n",
      "            RuntimeError: If the model was never compiled.\n",
      "            ValueError: In case of mismatch between the provided input data\n",
      "                and what the model expects.\n",
      "        \"\"\"\n",
      "        # Backwards compatibility\n",
      "        if batch_size is None and steps_per_epoch is None:\n",
      "            batch_size = 32\n",
      "        # Legacy support\n",
      "        if 'nb_epoch' in kwargs:\n",
      "            warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "                          'has been renamed `epochs`.', stacklevel=2)\n",
      "            epochs = kwargs.pop('nb_epoch')\n",
      "        if kwargs:\n",
      "            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n",
      "        if x is None and y is None and steps_per_epoch is None:\n",
      "            raise ValueError('If fitting from data tensors, '\n",
      "                             'you should specify the `steps_per_epoch` '\n",
      "                             'argument.')\n",
      "        # Validate user data.\n",
      "        x, y, sample_weights = self._standardize_user_data(\n",
      "            x, y,\n",
      "            sample_weight=sample_weight,\n",
      "            class_weight=class_weight,\n",
      "            batch_size=batch_size)\n",
      "        # Prepare validation data.\n",
      "        do_validation = False\n",
      "        if validation_data:\n",
      "            do_validation = True\n",
      "            if len(validation_data) == 2:\n",
      "                val_x, val_y = validation_data\n",
      "                val_sample_weight = None\n",
      "            elif len(validation_data) == 3:\n",
      "                val_x, val_y, val_sample_weight = validation_data\n",
      "            else:\n",
      "                raise ValueError('When passing validation_data, '\n",
      "                                 'it must contain 2 (x_val, y_val) '\n",
      "                                 'or 3 (x_val, y_val, val_sample_weights) '\n",
      "                                 'items, however it contains %d items' %\n",
      "                                 len(validation_data))\n",
      "\n",
      "            val_x, val_y, val_sample_weights = self._standardize_user_data(\n",
      "                val_x, val_y,\n",
      "                sample_weight=val_sample_weight,\n",
      "                batch_size=batch_size)\n",
      "            if self._uses_dynamic_learning_phase():\n",
      "                val_ins = val_x + val_y + val_sample_weights + [0.]\n",
      "            else:\n",
      "                val_ins = val_x + val_y + val_sample_weights\n",
      "\n",
      "        elif validation_split and 0. < validation_split < 1.:\n",
      "            if any(K.is_tensor(t) for t in x):\n",
      "                raise ValueError(\n",
      "                    'If your data is in the form of symbolic tensors, '\n",
      "                    'you cannot use `validation_split`.')\n",
      "            do_validation = True\n",
      "            if hasattr(x[0], 'shape'):\n",
      "                split_at = int(int(x[0].shape[0]) * (1. - validation_split))\n",
      "            else:\n",
      "                split_at = int(len(x[0]) * (1. - validation_split))\n",
      "            x, val_x = (slice_arrays(x, 0, split_at),\n",
      "                        slice_arrays(x, split_at))\n",
      "            y, val_y = (slice_arrays(y, 0, split_at),\n",
      "                        slice_arrays(y, split_at))\n",
      "            sample_weights, val_sample_weights = (\n",
      "                slice_arrays(sample_weights, 0, split_at),\n",
      "                slice_arrays(sample_weights, split_at))\n",
      "            if self._uses_dynamic_learning_phase():\n",
      "                val_ins = val_x + val_y + val_sample_weights + [0.]\n",
      "            else:\n",
      "                val_ins = val_x + val_y + val_sample_weights\n",
      "\n",
      "        elif validation_steps:\n",
      "            do_validation = True\n",
      "            if self._uses_dynamic_learning_phase():\n",
      "                val_ins = [0.]\n",
      "\n",
      "        # Prepare input arrays and training function.\n",
      "        if self._uses_dynamic_learning_phase():\n",
      "            ins = x + y + sample_weights + [1.]\n",
      "        else:\n",
      "            ins = x + y + sample_weights\n",
      "        self._make_train_function()\n",
      "        f = self.train_function\n",
      "\n",
      "        # Prepare display labels.\n",
      "        out_labels = self.metrics_names\n",
      "\n",
      "        if do_validation:\n",
      "            self._make_test_function()\n",
      "            val_f = self.test_function\n",
      "            callback_metrics = copy.copy(out_labels) + [\n",
      "                'val_' + n for n in out_labels]\n",
      "        else:\n",
      "            callback_metrics = copy.copy(out_labels)\n",
      "            val_f = None\n",
      "            val_ins = []\n",
      "\n",
      "        # Delegate logic to `fit_loop`.\n",
      "        return training_arrays.fit_loop(self, f, ins,\n",
      "                                        out_labels=out_labels,\n",
      "                                        batch_size=batch_size,\n",
      "                                        epochs=epochs,\n",
      "                                        verbose=verbose,\n",
      "                                        callbacks=callbacks,\n",
      "                                        val_f=val_f,\n",
      "                                        val_ins=val_ins,\n",
      "                                        shuffle=shuffle,\n",
      "                                        callback_metrics=callback_metrics,\n",
      "                                        initial_epoch=initial_epoch,\n",
      "                                        steps_per_epoch=steps_per_epoch,\n",
      "                                        validation_steps=validation_steps)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import inspect\n",
    "lines = inspect.getsource(Sequential.fit)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
