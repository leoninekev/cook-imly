{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'enable_eager_execution'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ddf3115bdcc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'enable_eager_execution'"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__#current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__#old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input, Dense\n",
    "#from keras import metrics\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataset: (100000, 4)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.expanduser('C:\\\\Users\\\\might\\\\Desktop\\\\jupyter notebooks\\\\u.data')\n",
    "df = pd.read_csv(data_path, sep='\\t', header=None)#used for keras NN_SVD\n",
    "\n",
    "df2= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "\n",
    "#df[2]= df[2].apply(lambda x: int(x>3))#since in df, no. vals holding 1 ratings:6110, 2 ratings:11370, 3 ratings:27145, 4 ratings:34174, 5 ratings: 21201 \n",
    "\n",
    "shuffled_df = df.loc[np.random.randint(0, 100000, size=df.shape[0])]\n",
    "\n",
    "#sm_train_df = shuffled_df[:80000]#small dataframe, trimming data points to random 2000 datapoints\n",
    "#test_df= shuffled_df[-20000:]\n",
    "\n",
    "#print('new trimmed dataset:', sm_train_df.shape, '\\nnew test dataset:', test_df.shape)\n",
    "print('shape of dataset:', shuffled_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train_user_in: (100000, 944) \n",
      "shape of x_train_movie_in: (100000, 1683) \n",
      "shape of x_train_user_in: (100000,)\n"
     ]
    }
   ],
   "source": [
    "##code from previous keras implementation\n",
    "x_train_user_in = to_categorical(shuffled_df[0])#contains the one-hot encoded user_id data, shaped (batch_size, max(sm_df[0]))\n",
    "x_train_movie_in = to_categorical(shuffled_df[1])#contains the one-hot encoded movie_id data, shaped (batch_size, max(sm_df[1]))\n",
    "\n",
    "y_ratings= shuffled_df[2]\n",
    "print('shape of x_train_user_in:', x_train_user_in.shape,'\\nshape of x_train_movie_in:', x_train_movie_in.shape,\n",
    "      '\\nshape of x_train_user_in:',y_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_user_in[0]#derived from df[0] --correspoding to first user in uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([455], dtype=int64),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x_train_user_in[0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "##drawing code from deepCTR\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.inputs import SparseFeat, DenseFeat, get_fixlen_feature_names, VarLenSparseFeat\n",
    "\n",
    "df2= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\"]\n",
    "y= ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0      196       242       3  881250949\n",
       "1      186       302       3  891717742\n",
       "2       22       377       1  878887116\n",
       "3      244        51       2  880606923"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(np.sort(np.unique(df2['user_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    195\n",
       "1    185\n",
       "2     21\n",
       "3    243\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df2[feat] = lbe.fit_transform(df2[feat])\n",
    "    ##unique features for each sparse field\n",
    "    \n",
    "df2[sparse_features[1]].head(4)#derived from df2['user_id'] --correspoding to first user in uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, df2[feat].nunique()) for feat in sparse_features]\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse feats & Dense feats:\n",
      " [SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)] \n",
      " [SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]\n"
     ]
    }
   ],
   "source": [
    "print('Sparse feats & Dense feats:\\n',linear_feature_columns,'\\n', dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import namedtuple#testing inheritence with namedtuple class & super() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#person = namedtuple('Person','name age gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('name', '_1', 'gender', 'age', '_4')\n"
     ]
    }
   ],
   "source": [
    "#with_class = namedtuple('Person', 'name class gender age gender', rename=True)\n",
    "#print(with_class._fields)\n",
    "\n",
    "#two_ages = collections.namedtuple('Person', 'name age gender age', rename=True)\n",
    "#print two_ages._fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MyParentClass():\n",
    " #   def __init__(self):\n",
    "  #      self.var = 7\n",
    "  #  def printout(self):\n",
    "   #     print('output is :', self.var)\n",
    "\n",
    "#class SubClass(MyParentClass):\n",
    " #   def __init__(self):\n",
    "  #      super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a= SubClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output is : 7\n"
     ]
    }
   ],
   "source": [
    "#a.printout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True),\n",
       " SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True),\n",
       " SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True),\n",
       " SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features = build_input_features(linear_feature_columns + dnn_feature_columns)\n",
    "feature_columns= linear_feature_columns + dnn_feature_columns\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Following is line by line execution of deepctr.models.DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepFM(linear_feature_columns, dnn_feature_columns, embedding_size=8, use_fm=True, dnn_hidden_units=(128, 128),\n",
    "#l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, init_std=0.0001, seed=1024,dnn_dropout=0, dnn_activation='relu',\n",
    "#dnn_use_bn=False, task='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features[feature_columns[0].name] = Input(shape=(1,), name='' + feature_columns[0].name, dtype=feature_columns[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'movie_id_1:0' shape=(None, 1) dtype=int32>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[feature_columns[0].name]#Inpute layer corresponding to 1st SparseFeat object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tensorflow.python.keras.layers import  Embedding, Input, Flatten\n",
    "#from tensorflow.python.keras.regularizers import l2\n",
    "\n",
    "#from .layers.sequence import SequencePoolingLayer\n",
    "\n",
    "\n",
    "def build_input_features(feature_columns, include_varlen=True, mask_zero=True, prefix='',include_fixlen=True):\n",
    "    input_features = OrderedDict()\n",
    "    if include_fixlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,SparseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(1,), name=prefix+fc.name, dtype=fc.dtype)\n",
    "            elif isinstance(fc,DenseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "    if include_varlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,VarLenSparseFeat):\n",
    "                input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix + 'seq_' + fc.name,\n",
    "                                                      dtype=fc.dtype)\n",
    "        if not mask_zero:\n",
    "            for fc in feature_columns:\n",
    "                input_features[fc.name+\"_seq_length\"] = Input(shape=(\n",
    "                    1,), name=prefix + 'seq_length_' + fc.name)\n",
    "                input_features[fc.name+\"_seq_max_length\"] = fc.maxlen\n",
    "\n",
    "\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id',\n",
       "              <tf.Tensor 'movie_id_11:0' shape=(None, 1) dtype=int32>),\n",
       "             ('user_id',\n",
       "              <tf.Tensor 'user_id_9:0' shape=(None, 1) dtype=int32>)])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features= build_input_features(feature_columns)\n",
    "features #Input feature layers obatined from SparseFeat parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'movie_id_5:0' shape=(None, 1) dtype=int32>,\n",
       " <tf.Tensor 'user_id_3:0' shape=(None, 1) dtype=int32>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_list = list(features.values())\n",
    "inputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True),\n",
       " SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "\n",
    "sparse_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.initializers import RandomNormal\n",
    "\n",
    "def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed, l2_reg,\n",
    "                          prefix='sparse_', seq_mask_zero=True):\n",
    "    if embedding_size == 'auto':\n",
    "        print(\"Notice:Do not use auto embedding in models other than DCN\")\n",
    "        sparse_embedding = {feat.embedding_name: Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                 embeddings_initializer=RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=l2(l2_reg),\n",
    "                                                 name=prefix + '_emb_' + feat.name) for feat in\n",
    "                            sparse_feature_columns}\n",
    "    else:\n",
    "\n",
    "        sparse_embedding = {feat.embedding_name: Embedding(feat.dimension, embedding_size,\n",
    "                                                 embeddings_initializer=RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=l2(\n",
    "                                                     l2_reg),\n",
    "                                                 name=prefix + '_emb_'  + feat.name) for feat in\n",
    "                            sparse_feature_columns}\n",
    "\n",
    "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
    "        for feat in varlen_sparse_feature_columns:\n",
    "            # if feat.name not in sparse_embedding:\n",
    "            if embedding_size == \"auto\":\n",
    "                sparse_embedding[feat.embedding_name] = Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                        embeddings_initializer=RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=l2(\n",
    "                                                            l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "            else:\n",
    "                sparse_embedding[feat.embedding_name] = Embedding(feat.dimension, embedding_size,\n",
    "                                                        embeddings_initializer=RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=l2(\n",
    "                                                            l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "\n",
    "    return sparse_embedding\n",
    "\n",
    "def create_embedding_matrix(feature_columns,l2_reg,init_std,seed,embedding_size, prefix=\"\",seq_mask_zero=True):\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed,\n",
    "                                                 l2_reg, prefix=prefix + 'sparse',seq_mask_zero=seq_mask_zero)\n",
    "    return sparse_emb_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <tensorflow.python.keras.layers.embeddings.Embedding at 0x254edbea208>,\n",
       " 'user_id': <tensorflow.python.keras.layers.embeddings.Embedding at 0x254eba407f0>}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line 3 in input_from_feature_columns( ) function\n",
    "embedding_dict = create_embedding_matrix(dnn_feature_columns, 0.00001, 0.0001, 1024, 8, prefix='',seq_mask_zero=True)\n",
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line 4 in input_from_feature_columns( ) function\n",
    "def embedding_lookup(sparse_embedding_dict,sparse_input_dict,sparse_feature_columns,return_feat_list=(), mask_feat_list=()):\n",
    "    embedding_vec_list = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if len(return_feat_list) == 0  or feature_name in return_feat_list and fc.embedding:\n",
    "            #if fc.use_hash:\n",
    "            #    lookup_idx = Hash(fc.dimension,mask_zero=(feature_name in mask_feat_list))(sparse_input_dict[feature_name])\n",
    "            lookup_idx = sparse_input_dict[feature_name]\n",
    "\n",
    "            embedding_vec_list.append(sparse_embedding_dict[embedding_name](lookup_idx))\n",
    "\n",
    "    return embedding_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_feature_columns:\n",
      " [SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]\n",
      "\n",
      "sparse_embedding_list:\n",
      " [<tf.Tensor 'sparse_emb_movie_id/Identity:0' shape=(None, 1, 8) dtype=float32>, <tf.Tensor 'sparse_emb_user_id/Identity:0' shape=(None, 1, 8) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat) and x.embedding, dnn_feature_columns)) if dnn_feature_columns else []\n",
    "\n",
    "#features: OrderedDict([('movie_id',<tf.Tensor 'movie_id_7:0' shape=(None, 1) dtype=int32>), ...])\n",
    "# output from build_input_features( ), line 1 in DeepFM() function\n",
    "\n",
    "sparse_embedding_list = embedding_lookup(embedding_dict, features, sparse_feature_columns)\n",
    "print('sparse_feature_columns:\\n',sparse_feature_columns)\n",
    "print('\\nsparse_embedding_list:\\n',sparse_embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line 6 in input_from_Feature_Columns( )\n",
    "def get_dense_input(features,feature_columns):\n",
    "    dense_feature_columns = list(filter(lambda x:isinstance(x,DenseFeat),feature_columns)) if feature_columns else []\n",
    "    dense_input_list = []\n",
    "    for fc in dense_feature_columns:\n",
    "        dense_input_list.append(features[fc.name])\n",
    "    return dense_input_list\n",
    "\n",
    "dense_value_list = get_dense_input(features,feature_columns)\n",
    "dense_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id',\n",
       "              <tf.Tensor 'movie_id_7:0' shape=(None, 1) dtype=int32>),\n",
       "             ('user_id',\n",
       "              <tf.Tensor 'user_id_5:0' shape=(None, 1) dtype=int32>)])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from deepctr.inputs \n",
    "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
    "    varlen_embedding_vec_dict = {}\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if fc.use_hash:\n",
    "            lookup_idx = Hash(fc.dimension, mask_zero=True)(sequence_input_dict[feature_name])\n",
    "        else:\n",
    "            lookup_idx = sequence_input_dict[feature_name]\n",
    "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
    "\n",
    "    return varlen_embedding_vec_dict\n",
    "\n",
    "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns):\n",
    "    pooling_vec_list = []\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        combiner = fc.combiner\n",
    "        feature_length_name = feature_name + '_seq_length'\n",
    "        if feature_length_name in features:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n",
    "            [embedding_dict[feature_name], features[feature_length_name]])\n",
    "        else:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n",
    "            embedding_dict[feature_name])\n",
    "        pooling_vec_list.append(vec)\n",
    "    return pooling_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varlen_sparse_feature_columns:  []\n",
      "\n",
      "sequence_embed_dict:  {}\n",
      "\n",
      "sequence_embed_list:  []\n"
     ]
    }
   ],
   "source": [
    "#line 2 in input_from_featre_columns( )\n",
    "varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "print('varlen_sparse_feature_columns: ', varlen_sparse_feature_columns)\n",
    "\n",
    "sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,varlen_sparse_feature_columns)\n",
    "print('\\nsequence_embed_dict: ',sequence_embed_dict)\n",
    "\n",
    "sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, varlen_sparse_feature_columns)\n",
    "print('\\nsequence_embed_list: ', sequence_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line 3 in DeepFM( ) function\n",
    "#features: output from build_input_features( ) from line 1 in DeepFM()\n",
    "#feature_columns: dnn_feature_columns\n",
    "def input_from_feature_columns(features,feature_columns, embedding_size, l2_reg, init_std, seed,prefix='',seq_mask_zero=True,support_dense=True):\n",
    "\n",
    "\n",
    "    sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat),feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "    embedding_dict = create_embedding_matrix(feature_columns,l2_reg,init_std,seed,embedding_size, prefix=prefix,seq_mask_zero=seq_mask_zero)\n",
    "    \n",
    "    sparse_embedding_list = embedding_lookup(\n",
    "        embedding_dict, features, sparse_feature_columns)\n",
    "    dense_value_list = get_dense_input(features,feature_columns)\n",
    "    if not support_dense and len(dense_value_list) >0:\n",
    "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,varlen_sparse_feature_columns)\n",
    "    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, varlen_sparse_feature_columns)\n",
    "    sparse_embedding_list += sequence_embed_list\n",
    "\n",
    "    return sparse_embedding_list, dense_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embedding_list:  [<tf.Tensor 'sparse_emb_movie_id_1/Identity:0' shape=(None, 1, 8) dtype=float32>, <tf.Tensor 'sparse_emb_user_id_1/Identity:0' shape=(None, 1, 8) dtype=float32>]\n",
      "\n",
      "dense_value_list:  []\n"
     ]
    }
   ],
   "source": [
    "sparse_embedding_list, dense_value_list = input_from_feature_columns(features,dnn_feature_columns,\n",
    "     embedding_size=8, l2_reg=0.00001,init_std=0.0001,seed=1024)\n",
    "print('sparse_embedding_list: ', sparse_embedding_list)\n",
    "print('\\ndense_value_list: ', dense_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now define a SVD_NN() along the lines with DeepFM() function, using/customising Some of the above methods/functionalities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tensorflow.python.keras.layers import  Embedding, Input, Dense\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from deepctr.models import DeepFM\n",
    "\n",
    "from deepctr.inputs import SparseFeat#DenseFeat, get_fixlen_feature_names, VarLenSparseFeat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#insert this to from .layers.sequence import SequencePoolingLayer\n",
    "def build_input_features(feature_columns, include_varlen=True, prefix='',include_fixlen=True):\n",
    "    input_features = OrderedDict()\n",
    "    if include_fixlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,SparseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(fc.dimension,), name=prefix+fc.name, dtype=fc.dtype)# Here shape changed from shape=(1,) to shape= (fc.dimesnion,)\n",
    "            #elif isinstance(fc,DenseFeat):\n",
    "             #   input_features[fc.name] = Input(\n",
    "              #      shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "                \n",
    "    return input_features\n",
    "\n",
    "\n",
    "def NN_svd(fixlen_feature_columns, num_factors=100):\n",
    "    features = build_input_features(fixlen_feature_columns)\n",
    "    \n",
    "    inputs= list(features.values())\n",
    "    \n",
    "    hid_layer_1= Dense(num_factors)(inputs[0])\n",
    "    hid_layer_2= Dense(num_factors)(inputs[1])\n",
    "    \n",
    "    merge_layer = tf.keras.layers.dot([hid_layer_1, hid_layer_2], axes=1)\n",
    "    \n",
    "    predictions = Dense(5, activation='softmax')(merge_layer)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id',\n",
       "              <tf.Tensor 'movie_id_11:0' shape=(None, 1) dtype=int32>),\n",
       "             ('user_id',\n",
       "              <tf.Tensor 'user_id_9:0' shape=(None, 1) dtype=int32>)])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature= build_input_features(fixlen_feature_columns)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.expanduser('C:\\\\Users\\\\might\\\\Desktop\\\\jupyter notebooks\\\\u.data')\n",
    "df2= pd.read_csv(data_path, sep='\\t',names= 'user_id\tmovie_id\trating\ttimestamp'.split('\t'))#, header=None)#used for DeepCTR\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\"]\n",
    "y= ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseFeat(name='movie_id', dimension=1682, use_hash=False, dtype='int32', embedding_name='movie_id', embedding=True), SparseFeat(name='user_id', dimension=943, use_hash=False, dtype='int32', embedding_name='user_id', embedding=True)]\n"
     ]
    }
   ],
   "source": [
    "#This counts unique values & encodes existing value to new lable in progression\n",
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        df2[feat] = lbe.fit_transform(df2[feat])\n",
    "    ##unique features for each sparse field\n",
    "    \n",
    "fixlen_feature_columns = [SparseFeat(feat, df2[feat].nunique()) for feat in sparse_features]\n",
    "print(fixlen_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train, test = train_test_split(df2, test_size=0.2)\n",
    "\n",
    "train_model_input = [to_categorical(train[fc.name].values, num_classes= fc.dimension) for fc in fixlen_feature_columns]#includes values from only data[user_id], data[movie_id]\n",
    "test_model_input = [to_categorical(train[fc.name].values, num_classes= fc.dimension) for fc in fixlen_feature_columns]#includes values from only data[user_id], data[movie_id]\n",
    "\n",
    "\n",
    "#abc= to_categorical(train_model_input[0], num_classes= fixlen_feature_columns[0].dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_id (InputLayer)           [(None, 1682)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_id (InputLayer)            [(None, 943)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          168300      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100)          94400       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 5)            10          dot_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 262,710\n",
      "Trainable params: 262,710\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NN_svd(fixlen_feature_columns)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc= model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 100)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'categorical_crossentropy',optimizer=\"sgd\", metrics=['mae', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical labels for softmax\n",
    "y_cat = to_categorical(train[y])\n",
    "\n",
    "y_cat= y_cat[:,1:]#stripping 0th column to shape (None,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/8\n",
      "64000/64000 - 2s - loss: 1.4678 - mae: 0.2994 - accuracy: 0.3417 - val_loss: 1.4694 - val_mae: 0.2995 - val_accuracy: 0.3406\n",
      "Epoch 2/8\n",
      "64000/64000 - 2s - loss: 1.4675 - mae: 0.2993 - accuracy: 0.3417 - val_loss: 1.4693 - val_mae: 0.2994 - val_accuracy: 0.3406\n",
      "Epoch 3/8\n",
      "64000/64000 - 2s - loss: 1.4674 - mae: 0.2992 - accuracy: 0.3417 - val_loss: 1.4691 - val_mae: 0.2993 - val_accuracy: 0.3406\n",
      "Epoch 4/8\n",
      "64000/64000 - 2s - loss: 1.4672 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4690 - val_mae: 0.2993 - val_accuracy: 0.3406\n",
      "Epoch 5/8\n",
      "64000/64000 - 2s - loss: 1.4670 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4688 - val_mae: 0.2993 - val_accuracy: 0.3406\n",
      "Epoch 6/8\n",
      "64000/64000 - 2s - loss: 1.4668 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4686 - val_mae: 0.2992 - val_accuracy: 0.3406\n",
      "Epoch 7/8\n",
      "64000/64000 - 2s - loss: 1.4665 - mae: 0.2991 - accuracy: 0.3417 - val_loss: 1.4683 - val_mae: 0.2992 - val_accuracy: 0.3406\n",
      "Epoch 8/8\n",
      "64000/64000 - 2s - loss: 1.4662 - mae: 0.2990 - accuracy: 0.3417 - val_loss: 1.4680 - val_mae: 0.2993 - val_accuracy: 0.3406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24b5aa655c0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_model_input, y_cat, batch_size=64, epochs=8, verbose=2, validation_split=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 5 which is outside the valid range of [0, 5).  Label values: 4 4 3 5 5 4 4 3 4 4 4 3 3 5 5 2 4 3 4 5 4 4 3 4 5 4 4 5 3 3 2 4 5 5 3 4 4 3 4 5 5 5 4 4 5 2 5 4 3 3 4 5 2 4 1 4 4 5 5 4 3 4 4 2\n\t [[node loss/dense_8_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at c:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_142306]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-9d81788568e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_model_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mc:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Received a label value of 5 which is outside the valid range of [0, 5).  Label values: 4 4 3 5 5 4 4 3 4 4 4 3 3 5 5 2 4 3 4 5 4 4 3 4 5 4 4 5 3 3 2 4 5 5 3 4 4 3 4 5 5 5 4 4 5 2 5 4 3 3 4 5 2 4 1 4 4 5 5 4 3 4 4 2\n\t [[node loss/dense_8_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at c:\\users\\might\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_142306]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_model_input, train[y].values, batch_size=64, epochs=3, verbose=2, validation_split=0.2,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
