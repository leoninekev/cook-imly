{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments using IMLY ###\n",
    "\n",
    "This notebook contains experimental runs of IMLY with different datasets.  \n",
    "The readings of these experiments can be referred to in this [sheet](https://docs.google.com/spreadsheets/d/1E5jcq2w42gN8bMIaeaRJpAdhgSVN-2XDJ_YTHe4qfwY/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #1\n",
    "\n",
    "#### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "266/266 [==============================] - ETA:  - 0s 101us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "# Y = preprocessing.normalize(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #2\n",
    "\n",
    "#### UCI Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "  round_epochs            val_loss                loss  lr units batch_size  \\\n",
      "0           10  0.6475951799056815  0.6550543823757687  30     1         10   \n",
      "\n",
      "  epochs weight_regulizer emb_output_dims optimizer               losses  \\\n",
      "0     10             None            None      adam  binary_crossentropy   \n",
      "\n",
      "  activation          model_name  \n",
      "0    sigmoid  LogisticRegression  \n",
      "794/794 [==============================] - ETA:  - 0s 79us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"uci_abalone\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\" if path.exists(\"../data/abalone.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #3\n",
    "\n",
    "#### UCI Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "  round_epochs            val_loss                loss  lr units batch_size  \\\n",
      "0          200  0.7741430004437765  0.7907322347164154  30     1         10   \n",
      "\n",
      "  epochs weight_regulizer emb_output_dims optimizer               losses  \\\n",
      "0    200             None            None      adam  binary_crossentropy   \n",
      "\n",
      "  activation          model_name  \n",
      "0    sigmoid  LogisticRegression  \n",
      "60/60 [==============================] - ETA:  - 0s 261us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "params = {\n",
    "    'epochs': 200\n",
    "}\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from else\n",
      "input from __call__  --  Tensor(\"dense_1_input:0\", shape=(?, 4), dtype=float32)\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6207 - acc: 0.468 - 0s 4ms/step - loss: 1.3985 - acc: 0.5500\n",
      "60/60 [==============================] - ETA:  - 0s 931us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4666666626930237"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "np.random.seed(7)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,input_dim=4,activation='sigmoid'))\n",
    "\n",
    "    # Compile the model #\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    \n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "scores = model.score(x_test, y_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input from __call__  --  Tensor(\"dense_2_input_1:0\", shape=(?, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x175431b9f28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model.__call__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #4\n",
    "\n",
    "#### UCI Adult salary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x0000022E25D08160>\n",
      "From try --  <function glm at 0x0000022E23B86F28>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "from elif\n",
      "<keras.engine.sequential.Sequential object at 0x0000022EFAA60E10>\n",
      "From except --  <function glm at 0x0000022E23B86F28>\n",
      "Epoch 1/10\n",
      "18088/18088 [==============================] - ETA: 1:36 - loss: 4.5332 - acc: 0.718 - ETA: 6s - loss: 3.6769 - acc: 0.7719  - ETA: 3s - loss: 3.7101 - acc: 0.769 - ETA: 2s - loss: 3.7337 - acc: 0.768 - ETA: 2s - loss: 3.6822 - acc: 0.771 - ETA: 1s - loss: 3.7573 - acc: 0.766 - ETA: 1s - loss: 3.7464 - acc: 0.767 - ETA: 1s - loss: 3.7853 - acc: 0.765 - ETA: 1s - loss: 3.8051 - acc: 0.763 - ETA: 1s - loss: 3.8077 - acc: 0.763 - ETA: 1s - loss: 3.8414 - acc: 0.761 - ETA: 0s - loss: 3.8220 - acc: 0.762 - ETA: 0s - loss: 3.8507 - acc: 0.761 - ETA: 0s - loss: 3.8475 - acc: 0.761 - ETA: 0s - loss: 3.8289 - acc: 0.762 - ETA: 0s - loss: 3.8144 - acc: 0.763 - ETA: 0s - loss: 3.8425 - acc: 0.761 - ETA: 0s - loss: 3.8558 - acc: 0.760 - ETA: 0s - loss: 3.8683 - acc: 0.760 - ETA: 0s - loss: 3.8731 - acc: 0.759 - ETA: 0s - loss: 3.8738 - acc: 0.759 - ETA: 0s - loss: 3.8766 - acc: 0.759 - ETA: 0s - loss: 3.8831 - acc: 0.759 - 2s 85us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 2/10\n",
      "18088/18088 [==============================] - ETA: 8s - loss: 2.0148 - acc: 0.875 - ETA: 1s - loss: 3.8989 - acc: 0.758 - ETA: 1s - loss: 3.8647 - acc: 0.760 - ETA: 1s - loss: 3.9385 - acc: 0.755 - ETA: 1s - loss: 3.9161 - acc: 0.757 - ETA: 1s - loss: 3.9711 - acc: 0.753 - ETA: 0s - loss: 3.9173 - acc: 0.757 - ETA: 0s - loss: 3.9049 - acc: 0.757 - ETA: 0s - loss: 3.9260 - acc: 0.756 - ETA: 0s - loss: 3.9077 - acc: 0.757 - ETA: 0s - loss: 3.9347 - acc: 0.755 - ETA: 0s - loss: 3.9349 - acc: 0.755 - ETA: 0s - loss: 3.9341 - acc: 0.755 - ETA: 0s - loss: 3.9239 - acc: 0.756 - ETA: 0s - loss: 3.9277 - acc: 0.756 - ETA: 0s - loss: 3.9140 - acc: 0.757 - ETA: 0s - loss: 3.8853 - acc: 0.758 - ETA: 0s - loss: 3.8964 - acc: 0.758 - ETA: 0s - loss: 3.8959 - acc: 0.758 - ETA: 0s - loss: 3.9211 - acc: 0.756 - ETA: 0s - loss: 3.9029 - acc: 0.757 - ETA: 0s - loss: 3.8856 - acc: 0.758 - ETA: 0s - loss: 3.8753 - acc: 0.759 - 1s 79us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 3/10\n",
      "18088/18088 [==============================] - ETA: 0s - loss: 3.5258 - acc: 0.781 - ETA: 2s - loss: 3.8280 - acc: 0.762 - ETA: 1s - loss: 3.9851 - acc: 0.752 - ETA: 1s - loss: 4.0379 - acc: 0.749 - ETA: 1s - loss: 3.9821 - acc: 0.752 - ETA: 1s - loss: 3.9547 - acc: 0.754 - ETA: 1s - loss: 3.8826 - acc: 0.759 - ETA: 1s - loss: 3.8892 - acc: 0.758 - ETA: 1s - loss: 3.8887 - acc: 0.758 - ETA: 1s - loss: 3.8986 - acc: 0.758 - ETA: 1s - loss: 3.9023 - acc: 0.757 - ETA: 1s - loss: 3.9002 - acc: 0.758 - ETA: 1s - loss: 3.9089 - acc: 0.757 - ETA: 0s - loss: 3.9061 - acc: 0.757 - ETA: 0s - loss: 3.9108 - acc: 0.757 - ETA: 0s - loss: 3.9074 - acc: 0.757 - ETA: 0s - loss: 3.9008 - acc: 0.758 - ETA: 0s - loss: 3.8984 - acc: 0.758 - ETA: 0s - loss: 3.8928 - acc: 0.758 - ETA: 0s - loss: 3.8867 - acc: 0.758 - ETA: 0s - loss: 3.8915 - acc: 0.758 - ETA: 0s - loss: 3.8864 - acc: 0.758 - ETA: 0s - loss: 3.9044 - acc: 0.757 - ETA: 0s - loss: 3.9016 - acc: 0.757 - ETA: 0s - loss: 3.8968 - acc: 0.758 - ETA: 0s - loss: 3.8904 - acc: 0.758 - 2s 89us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 4/10\n",
      "18088/18088 [==============================] - ETA: 8s - loss: 4.5332 - acc: 0.718 - ETA: 1s - loss: 3.4601 - acc: 0.785 - ETA: 1s - loss: 3.9438 - acc: 0.755 - ETA: 1s - loss: 3.9360 - acc: 0.755 - ETA: 1s - loss: 3.8753 - acc: 0.759 - ETA: 1s - loss: 3.7756 - acc: 0.765 - ETA: 1s - loss: 3.8342 - acc: 0.762 - ETA: 1s - loss: 3.8674 - acc: 0.760 - ETA: 0s - loss: 3.7990 - acc: 0.764 - ETA: 0s - loss: 3.8022 - acc: 0.764 - ETA: 0s - loss: 3.7966 - acc: 0.764 - ETA: 0s - loss: 3.8538 - acc: 0.760 - ETA: 0s - loss: 3.8761 - acc: 0.759 - ETA: 0s - loss: 3.9021 - acc: 0.757 - ETA: 0s - loss: 3.9061 - acc: 0.757 - ETA: 0s - loss: 3.9036 - acc: 0.757 - ETA: 0s - loss: 3.9055 - acc: 0.757 - ETA: 0s - loss: 3.8988 - acc: 0.758 - ETA: 0s - loss: 3.8923 - acc: 0.758 - ETA: 0s - loss: 3.8835 - acc: 0.759 - ETA: 0s - loss: 3.8965 - acc: 0.758 - ETA: 0s - loss: 3.9004 - acc: 0.758 - ETA: 0s - loss: 3.8838 - acc: 0.759 - 1s 80us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 5/10\n",
      "18088/18088 [==============================] - ETA: 0s - loss: 3.5258 - acc: 0.781 - ETA: 2s - loss: 4.8432 - acc: 0.699 - ETA: 1s - loss: 4.0715 - acc: 0.747 - ETA: 1s - loss: 4.2164 - acc: 0.738 - ETA: 1s - loss: 4.1048 - acc: 0.745 - ETA: 1s - loss: 4.1190 - acc: 0.744 - ETA: 1s - loss: 4.0333 - acc: 0.749 - ETA: 1s - loss: 4.0295 - acc: 0.750 - ETA: 0s - loss: 4.0085 - acc: 0.751 - ETA: 0s - loss: 3.9979 - acc: 0.752 - ETA: 0s - loss: 3.9780 - acc: 0.753 - ETA: 0s - loss: 3.9284 - acc: 0.756 - ETA: 0s - loss: 3.9220 - acc: 0.756 - ETA: 0s - loss: 3.9300 - acc: 0.756 - ETA: 0s - loss: 3.9339 - acc: 0.755 - ETA: 0s - loss: 3.9098 - acc: 0.757 - ETA: 0s - loss: 3.8798 - acc: 0.759 - ETA: 0s - loss: 3.8786 - acc: 0.759 - ETA: 0s - loss: 3.8993 - acc: 0.758 - ETA: 0s - loss: 3.8950 - acc: 0.758 - ETA: 0s - loss: 3.8950 - acc: 0.758 - ETA: 0s - loss: 3.8903 - acc: 0.758 - 1s 74us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 6/10\n",
      "18088/18088 [==============================] - ETA: 8s - loss: 4.5332 - acc: 0.718 - ETA: 2s - loss: 4.0055 - acc: 0.751 - ETA: 1s - loss: 4.2755 - acc: 0.734 - ETA: 1s - loss: 4.1211 - acc: 0.744 - ETA: 1s - loss: 4.0079 - acc: 0.751 - ETA: 1s - loss: 3.9951 - acc: 0.752 - ETA: 1s - loss: 3.9144 - acc: 0.757 - ETA: 1s - loss: 3.9374 - acc: 0.755 - ETA: 1s - loss: 3.8795 - acc: 0.759 - ETA: 0s - loss: 3.9054 - acc: 0.757 - ETA: 0s - loss: 3.8868 - acc: 0.758 - ETA: 0s - loss: 3.8822 - acc: 0.759 - ETA: 0s - loss: 3.8846 - acc: 0.759 - ETA: 0s - loss: 3.8593 - acc: 0.760 - ETA: 0s - loss: 3.8870 - acc: 0.758 - ETA: 0s - loss: 3.8940 - acc: 0.758 - ETA: 0s - loss: 3.9003 - acc: 0.758 - ETA: 0s - loss: 3.9066 - acc: 0.757 - ETA: 0s - loss: 3.9087 - acc: 0.757 - ETA: 0s - loss: 3.9326 - acc: 0.756 - ETA: 0s - loss: 3.9180 - acc: 0.756 - ETA: 0s - loss: 3.9170 - acc: 0.757 - ETA: 0s - loss: 3.9198 - acc: 0.756 - ETA: 0s - loss: 3.9077 - acc: 0.757 - ETA: 0s - loss: 3.8943 - acc: 0.758 - 2s 87us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 7/10\n",
      "18088/18088 [==============================] - ETA: 0s - loss: 4.5332 - acc: 0.718 - ETA: 2s - loss: 4.6591 - acc: 0.710 - ETA: 1s - loss: 4.0583 - acc: 0.748 - ETA: 1s - loss: 3.7734 - acc: 0.765 - ETA: 1s - loss: 3.8103 - acc: 0.763 - ETA: 1s - loss: 3.8130 - acc: 0.763 - ETA: 1s - loss: 3.7470 - acc: 0.767 - ETA: 1s - loss: 3.7343 - acc: 0.768 - ETA: 1s - loss: 3.7034 - acc: 0.770 - ETA: 1s - loss: 3.7085 - acc: 0.769 - ETA: 0s - loss: 3.7581 - acc: 0.766 - ETA: 0s - loss: 3.7694 - acc: 0.766 - ETA: 0s - loss: 3.8013 - acc: 0.764 - ETA: 0s - loss: 3.8462 - acc: 0.761 - ETA: 0s - loss: 3.8359 - acc: 0.762 - ETA: 0s - loss: 3.8395 - acc: 0.761 - ETA: 0s - loss: 3.8653 - acc: 0.760 - ETA: 0s - loss: 3.8459 - acc: 0.761 - ETA: 0s - loss: 3.8540 - acc: 0.760 - ETA: 0s - loss: 3.8735 - acc: 0.759 - ETA: 0s - loss: 3.8768 - acc: 0.759 - ETA: 0s - loss: 3.8802 - acc: 0.759 - ETA: 0s - loss: 3.8851 - acc: 0.759 - ETA: 0s - loss: 3.8861 - acc: 0.758 - 1s 83us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 8/10\n",
      "18088/18088 [==============================] - ETA: 0s - loss: 2.5185 - acc: 0.843 - ETA: 1s - loss: 3.7777 - acc: 0.765 - ETA: 1s - loss: 3.7002 - acc: 0.770 - ETA: 1s - loss: 3.8389 - acc: 0.761 - ETA: 1s - loss: 3.8927 - acc: 0.758 - ETA: 1s - loss: 3.9841 - acc: 0.752 - ETA: 1s - loss: 3.9616 - acc: 0.754 - ETA: 1s - loss: 3.9240 - acc: 0.756 - ETA: 1s - loss: 3.9205 - acc: 0.756 - ETA: 1s - loss: 3.9288 - acc: 0.756 - ETA: 1s - loss: 3.9274 - acc: 0.756 - ETA: 0s - loss: 3.9181 - acc: 0.756 - ETA: 0s - loss: 3.9002 - acc: 0.758 - ETA: 0s - loss: 3.9099 - acc: 0.757 - ETA: 0s - loss: 3.9508 - acc: 0.754 - ETA: 0s - loss: 3.9193 - acc: 0.756 - ETA: 0s - loss: 3.9122 - acc: 0.757 - ETA: 0s - loss: 3.9372 - acc: 0.755 - ETA: 0s - loss: 3.9109 - acc: 0.757 - ETA: 0s - loss: 3.9299 - acc: 0.756 - ETA: 0s - loss: 3.9103 - acc: 0.757 - ETA: 0s - loss: 3.9177 - acc: 0.756 - ETA: 0s - loss: 3.8972 - acc: 0.758 - ETA: 0s - loss: 3.8891 - acc: 0.758 - ETA: 0s - loss: 3.8724 - acc: 0.759 - ETA: 0s - loss: 3.8845 - acc: 0.759 - ETA: 0s - loss: 3.9055 - acc: 0.757 - ETA: 0s - loss: 3.8925 - acc: 0.758 - 2s 96us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 9/10\n",
      "18088/18088 [==============================] - ETA: 8s - loss: 3.5258 - acc: 0.781 - ETA: 2s - loss: 4.0775 - acc: 0.747 - ETA: 1s - loss: 4.0647 - acc: 0.747 - ETA: 1s - loss: 3.9406 - acc: 0.755 - ETA: 1s - loss: 3.8721 - acc: 0.759 - ETA: 1s - loss: 3.8102 - acc: 0.763 - ETA: 1s - loss: 3.8605 - acc: 0.760 - ETA: 0s - loss: 3.8916 - acc: 0.758 - ETA: 0s - loss: 3.8926 - acc: 0.758 - ETA: 0s - loss: 3.8804 - acc: 0.759 - ETA: 0s - loss: 3.8848 - acc: 0.759 - ETA: 0s - loss: 3.8704 - acc: 0.759 - ETA: 0s - loss: 3.9132 - acc: 0.757 - ETA: 0s - loss: 3.8675 - acc: 0.760 - ETA: 0s - loss: 3.8958 - acc: 0.758 - ETA: 0s - loss: 3.8948 - acc: 0.758 - ETA: 0s - loss: 3.8828 - acc: 0.759 - ETA: 0s - loss: 3.8995 - acc: 0.758 - ETA: 0s - loss: 3.8834 - acc: 0.759 - ETA: 0s - loss: 3.8718 - acc: 0.759 - ETA: 0s - loss: 3.8857 - acc: 0.758 - 1s 73us/step - loss: 3.8789 - acc: 0.7593\n",
      "Epoch 10/10\n",
      "18088/18088 [==============================] - ETA: 0s - loss: 3.0221 - acc: 0.812 - ETA: 1s - loss: 3.7548 - acc: 0.767 - ETA: 1s - loss: 3.9036 - acc: 0.757 - ETA: 1s - loss: 3.8464 - acc: 0.761 - ETA: 1s - loss: 3.8632 - acc: 0.760 - ETA: 1s - loss: 3.8153 - acc: 0.763 - ETA: 0s - loss: 3.8375 - acc: 0.761 - ETA: 0s - loss: 3.8599 - acc: 0.760 - ETA: 0s - loss: 3.8869 - acc: 0.758 - ETA: 0s - loss: 3.8657 - acc: 0.760 - ETA: 0s - loss: 3.8917 - acc: 0.758 - ETA: 0s - loss: 3.8926 - acc: 0.758 - ETA: 0s - loss: 3.8981 - acc: 0.758 - ETA: 0s - loss: 3.8939 - acc: 0.758 - ETA: 0s - loss: 3.8745 - acc: 0.759 - ETA: 0s - loss: 3.8856 - acc: 0.758 - ETA: 0s - loss: 3.8932 - acc: 0.758 - ETA: 0s - loss: 3.8841 - acc: 0.759 - ETA: 0s - loss: 3.9041 - acc: 0.757 - ETA: 0s - loss: 3.8905 - acc: 0.758 - ETA: 0s - loss: 3.8785 - acc: 0.759 - 1s 71us/step - loss: 3.8789 - acc: 0.7593\n",
      "27134/27134 [==============================] - ETA: 13 - ETA: 0 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 26us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_adult_salary\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\" \", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)\n",
    "\n",
    "# Split the dataset into test and train datasets\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #5\n",
    "\n",
    "#### UCI Ad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x000001C8E1478668>\n",
      "From try --  <function glm at 0x000001C8E1236F28>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "from elif\n",
      "<keras.engine.sequential.Sequential object at 0x000001C8E1478940>\n",
      "From except --  <function glm at 0x000001C8E1236F28>\n",
      "Epoch 1/10\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.7525 - acc: 0.500 - ETA: 0s - loss: 0.7753 - acc: 0.589 - ETA: 0s - loss: 0.6939 - acc: 0.658 - 0s 274us/step - loss: 0.6936 - acc: 0.6564\n",
      "Epoch 2/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.4893 - acc: 0.781 - ETA: 0s - loss: 0.4560 - acc: 0.846 - 0s 135us/step - loss: 0.3951 - acc: 0.8791\n",
      "Epoch 3/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.3706 - acc: 0.906 - ETA: 0s - loss: 0.2988 - acc: 0.921 - 0s 116us/step - loss: 0.2698 - acc: 0.9374\n",
      "Epoch 4/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.1768 - acc: 0.968 - ETA: 0s - loss: 0.2124 - acc: 0.951 - 0s 116us/step - loss: 0.2017 - acc: 0.9512\n",
      "Epoch 5/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.1759 - acc: 0.937 - ETA: 0s - loss: 0.1599 - acc: 0.968 - 0s 116us/step - loss: 0.1606 - acc: 0.9629\n",
      "Epoch 6/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.1192 - acc: 0.968 - ETA: 0s - loss: 0.1271 - acc: 0.972 - 0s 116us/step - loss: 0.1356 - acc: 0.9692\n",
      "Epoch 7/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.0763 - acc: 1.000 - ETA: 0s - loss: 0.1314 - acc: 0.970 - 0s 99us/step - loss: 0.1184 - acc: 0.9745\n",
      "Epoch 8/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.1118 - acc: 0.968 - ETA: 0s - loss: 0.1037 - acc: 0.977 - 0s 116us/step - loss: 0.1057 - acc: 0.9735\n",
      "Epoch 9/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.1372 - acc: 0.937 - ETA: 0s - loss: 0.1164 - acc: 0.966 - 0s 116us/step - loss: 0.0963 - acc: 0.9767\n",
      "Epoch 10/10\n",
      "943/943 [==============================] - ETA: 0s - loss: 0.1180 - acc: 0.937 - ETA: 0s - loss: 0.0916 - acc: 0.980 - 0s 116us/step - loss: 0.0881 - acc: 0.9809\n",
      "1416/1416 [==============================] - ETA:  - ETA:  - 0s 77us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #6\n",
    "\n",
    "#### UCI Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset info #\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_mushroom\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields with missing values\n",
      "stalk-root\n",
      "2480\n",
      "30.53%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['classes', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "        'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "        'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "        'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color',\n",
    "        'population', 'habitat']\n",
    "url = \"../data/mushroom.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "print(\"Fields with missing values\")\n",
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")\n",
    "\n",
    "data = data[data[\"stalk-root\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "\n",
    "for col in names:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "# Split the dataset into test and train datasets #\n",
    "feature_list = names[1:23]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['classes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "3387/3387 [==============================] - ETA:  - ETA:  - 0s 37us/step\n"
     ]
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #7\n",
    "\n",
    "#### Covertype dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"covertype\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/covtype.data.csv\", delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "data = data[data[54].isin([1,2])]\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [11:53<00:00, 713.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "297085/297085 [==============================] - ETA: 5: - ETA: 1:25:1 - ETA: 5:04  - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 53s - ETA: 47 - ETA: 41 - ETA: 38 - ETA: 34 - ETA: 32 - ETA: 30 - ETA: 28 - ETA: 26 - ETA: 25 - ETA: 24 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 10s 34us/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #8\n",
    "\n",
    "#### TestData1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_1\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData1.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - 0s 125us/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\":10\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #9\n",
    "\n",
    "#### TestData2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_2\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData2.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique,count = np.unique(Y,return_counts=True)\n",
    "class1=count[0]/X.shape[0]*100\n",
    "class2=count[1]/X.shape[0]*100\n",
    "class_distribution = round(class1, 2)\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "480/480 [==============================] - ETA:  - 0s 98us/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #10\n",
    "\n",
    "#### UCI Airfoil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_airfoil\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_airfoil_self_noise.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "902/902 [==============================] - ETA:  - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #11\n",
    "\n",
    "#### UCI Auto-mpg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_auto_mpg\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_auto_mpg.csv\", delimiter=\",\", header=0, index_col='car name')\n",
    "data = data[data.horsepower != '?']\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:,1]\n",
    "X = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "236/236 [==============================] - ETA:  - 0s 97us/step\n"
     ]
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test bed ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LinearRegression'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "model_name = 'linear_regression'\n",
    "model_mappings = {\n",
    "    'linear_regression': 'LinearRegression',\n",
    "    'logistic_regression': 'LogisticRegression'\n",
    "}\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "for key, value in model_mappings.items():\n",
    "    if key == model_name:\n",
    "        name = value\n",
    "\n",
    "module = __import__('sklearn.linear_model', fromlist=[name])\n",
    "imported_module = getattr(module, name)\n",
    "model = imported_module\n",
    "\n",
    "primal_model = model()\n",
    "\n",
    "# Primal\n",
    "primal_model.fit(x_train, y_train)\n",
    "primal_model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# diabetes = datasets.load_diabetes()\n",
    "# sc = StandardScaler()\n",
    "# diabetes = sc.fit_transform(diabetes)\n",
    "#####\n",
    "# # Use only one feature\n",
    "# diabetes_X = diabetes.data\n",
    "# # sc = StandardScaler()\n",
    "# # diabetes.data = sc.fit_transform(diabetes.data)\n",
    "\n",
    "# X = diabetes.data\n",
    "# Y = diabetes.target\n",
    "#####\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "# # Split the data into training/testing sets\n",
    "# x_train = diabetes_X[:-20]\n",
    "# x_test = diabetes_X[-20:]\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# y_train = diabetes.target[:-20]\n",
    "# y_test = diabetes.target[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shakk\\\\Anaconda2\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\winmltools\\\\__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import winmltools\n",
    "winmltools.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "\n",
    "def f1(**kwargs):\n",
    "    params_json = json.load(open('../imly/architectures/sklearn/params.json'))\n",
    "    params = params_json['params'][kwargs['param_name']]\n",
    "    kwargs.setdefault('params', params)\n",
    "    kwargs.setdefault('x_train', np.array([[1], [2]]))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,\n",
    "                    input_dim=10,\n",
    "                    activation='linear'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_logarithmic_error',\n",
    "                  metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "176/176 [==============================] - ETA: 1s - loss: 0.2475 - mean_squared_error: 1.59 - 0s 2ms/step - loss: 0.1927 - mean_squared_error: 1.3051\n",
      "Epoch 2/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1404 - mean_squared_error: 0.65 - 0s 146us/step - loss: 0.1885 - mean_squared_error: 1.2741\n",
      "Epoch 3/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1835 - mean_squared_error: 1.26 - 0s 183us/step - loss: 0.1846 - mean_squared_error: 1.2461\n",
      "Epoch 4/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1397 - mean_squared_error: 1.04 - 0s 123us/step - loss: 0.1802 - mean_squared_error: 1.2163\n",
      "Epoch 5/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1908 - mean_squared_error: 1.39 - 0s 169us/step - loss: 0.1765 - mean_squared_error: 1.1895\n",
      "Epoch 6/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.2381 - mean_squared_error: 1.53 - 0s 104us/step - loss: 0.1727 - mean_squared_error: 1.1671\n",
      "Epoch 7/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0898 - mean_squared_error: 0.58 - 0s 173us/step - loss: 0.1682 - mean_squared_error: 1.1385\n",
      "Epoch 8/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1478 - mean_squared_error: 1.17 - 0s 184us/step - loss: 0.1653 - mean_squared_error: 1.1167\n",
      "Epoch 9/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1670 - mean_squared_error: 1.13 - 0s 144us/step - loss: 0.1619 - mean_squared_error: 1.0956\n",
      "Epoch 10/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1209 - mean_squared_error: 1.31 - 0s 181us/step - loss: 0.1583 - mean_squared_error: 1.0743\n",
      "Epoch 11/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1768 - mean_squared_error: 1.19 - 0s 141us/step - loss: 0.1547 - mean_squared_error: 1.0536\n",
      "Epoch 12/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1232 - mean_squared_error: 1.09 - 0s 162us/step - loss: 0.1512 - mean_squared_error: 1.0367\n",
      "Epoch 13/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1126 - mean_squared_error: 1.12 - 0s 180us/step - loss: 0.1478 - mean_squared_error: 1.0175\n",
      "Epoch 14/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1868 - mean_squared_error: 1.23 - 0s 168us/step - loss: 0.1449 - mean_squared_error: 1.0013\n",
      "Epoch 15/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1960 - mean_squared_error: 1.00 - 0s 164us/step - loss: 0.1421 - mean_squared_error: 0.9847\n",
      "Epoch 16/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1275 - mean_squared_error: 1.12 - 0s 183us/step - loss: 0.1389 - mean_squared_error: 0.9692\n",
      "Epoch 17/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1564 - mean_squared_error: 0.85 - 0s 152us/step - loss: 0.1362 - mean_squared_error: 0.9551\n",
      "Epoch 18/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1576 - mean_squared_error: 1.08 - 0s 140us/step - loss: 0.1340 - mean_squared_error: 0.9420\n",
      "Epoch 19/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0741 - mean_squared_error: 0.56 - 0s 171us/step - loss: 0.1317 - mean_squared_error: 0.9296\n",
      "Epoch 20/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1936 - mean_squared_error: 0.94 - 0s 121us/step - loss: 0.1298 - mean_squared_error: 0.9174\n",
      "Epoch 21/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0871 - mean_squared_error: 0.76 - 0s 147us/step - loss: 0.1279 - mean_squared_error: 0.9067\n",
      "Epoch 22/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1722 - mean_squared_error: 1.12 - 0s 191us/step - loss: 0.1263 - mean_squared_error: 0.8956\n",
      "Epoch 23/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0510 - mean_squared_error: 0.80 - 0s 205us/step - loss: 0.1247 - mean_squared_error: 0.8853\n",
      "Epoch 24/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1314 - mean_squared_error: 1.16 - 0s 107us/step - loss: 0.1234 - mean_squared_error: 0.8751\n",
      "Epoch 25/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.54 - 0s 183us/step - loss: 0.1218 - mean_squared_error: 0.8644\n",
      "Epoch 26/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1877 - mean_squared_error: 0.96 - 0s 160us/step - loss: 0.1207 - mean_squared_error: 0.8564\n",
      "Epoch 27/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1035 - mean_squared_error: 0.80 - 0s 145us/step - loss: 0.1196 - mean_squared_error: 0.8476\n",
      "Epoch 28/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1396 - mean_squared_error: 0.78 - 0s 101us/step - loss: 0.1183 - mean_squared_error: 0.8379\n",
      "Epoch 29/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0730 - mean_squared_error: 0.46 - 0s 181us/step - loss: 0.1172 - mean_squared_error: 0.8286\n",
      "Epoch 30/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.2055 - mean_squared_error: 1.03 - 0s 107us/step - loss: 0.1162 - mean_squared_error: 0.8207\n",
      "Epoch 31/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1675 - mean_squared_error: 1.03 - 0s 182us/step - loss: 0.1152 - mean_squared_error: 0.8126\n",
      "Epoch 32/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1300 - mean_squared_error: 0.85 - 0s 104us/step - loss: 0.1142 - mean_squared_error: 0.8052\n",
      "Epoch 33/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1117 - mean_squared_error: 0.75 - 0s 165us/step - loss: 0.1133 - mean_squared_error: 0.7973\n",
      "Epoch 34/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0996 - mean_squared_error: 0.54 - 0s 169us/step - loss: 0.1124 - mean_squared_error: 0.7893\n",
      "Epoch 35/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1278 - mean_squared_error: 0.81 - 0s 179us/step - loss: 0.1116 - mean_squared_error: 0.7824\n",
      "Epoch 36/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1606 - mean_squared_error: 0.95 - 0s 155us/step - loss: 0.1106 - mean_squared_error: 0.7742\n",
      "Epoch 37/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0989 - mean_squared_error: 0.72 - 0s 103us/step - loss: 0.1096 - mean_squared_error: 0.7662\n",
      "Epoch 38/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0945 - mean_squared_error: 0.82 - 0s 149us/step - loss: 0.1085 - mean_squared_error: 0.7579\n",
      "Epoch 39/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1103 - mean_squared_error: 0.83 - 0s 147us/step - loss: 0.1076 - mean_squared_error: 0.7507\n",
      "Epoch 40/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1005 - mean_squared_error: 0.61 - 0s 180us/step - loss: 0.1067 - mean_squared_error: 0.7425\n",
      "Epoch 41/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1602 - mean_squared_error: 0.75 - 0s 165us/step - loss: 0.1056 - mean_squared_error: 0.7337\n",
      "Epoch 42/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1598 - mean_squared_error: 0.89 - 0s 154us/step - loss: 0.1045 - mean_squared_error: 0.7257\n",
      "Epoch 43/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0669 - mean_squared_error: 0.55 - 0s 143us/step - loss: 0.1037 - mean_squared_error: 0.7188\n",
      "Epoch 44/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1228 - mean_squared_error: 0.63 - 0s 133us/step - loss: 0.1027 - mean_squared_error: 0.7106\n",
      "Epoch 45/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0930 - mean_squared_error: 0.66 - 0s 128us/step - loss: 0.1017 - mean_squared_error: 0.7041\n",
      "Epoch 46/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0881 - mean_squared_error: 0.66 - 0s 150us/step - loss: 0.1008 - mean_squared_error: 0.6973\n",
      "Epoch 47/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1249 - mean_squared_error: 0.80 - 0s 146us/step - loss: 0.1000 - mean_squared_error: 0.6911\n",
      "Epoch 48/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1541 - mean_squared_error: 0.85 - 0s 168us/step - loss: 0.0993 - mean_squared_error: 0.6840\n",
      "Epoch 49/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0909 - mean_squared_error: 0.56 - 0s 163us/step - loss: 0.0983 - mean_squared_error: 0.6771\n",
      "Epoch 50/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0640 - mean_squared_error: 0.42 - 0s 145us/step - loss: 0.0976 - mean_squared_error: 0.6714\n",
      "Epoch 51/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1016 - mean_squared_error: 0.51 - 0s 130us/step - loss: 0.0969 - mean_squared_error: 0.6661\n",
      "Epoch 52/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1316 - mean_squared_error: 0.84 - 0s 115us/step - loss: 0.0962 - mean_squared_error: 0.6602\n",
      "Epoch 53/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0470 - mean_squared_error: 0.53 - 0s 104us/step - loss: 0.0955 - mean_squared_error: 0.6557\n",
      "Epoch 54/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.95 - 0s 118us/step - loss: 0.0950 - mean_squared_error: 0.6509\n",
      "Epoch 55/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1292 - mean_squared_error: 0.60 - 0s 102us/step - loss: 0.0943 - mean_squared_error: 0.6454\n",
      "Epoch 56/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0807 - mean_squared_error: 0.49 - 0s 100us/step - loss: 0.0939 - mean_squared_error: 0.6411\n",
      "Epoch 57/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0803 - mean_squared_error: 0.52 - 0s 99us/step - loss: 0.0933 - mean_squared_error: 0.6371\n",
      "Epoch 58/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1189 - mean_squared_error: 0.80 - 0s 101us/step - loss: 0.0929 - mean_squared_error: 0.6332\n",
      "Epoch 59/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1451 - mean_squared_error: 0.71 - 0s 97us/step - loss: 0.0925 - mean_squared_error: 0.6289\n",
      "Epoch 60/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1369 - mean_squared_error: 0.65 - 0s 100us/step - loss: 0.0921 - mean_squared_error: 0.6253\n",
      "Epoch 61/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0865 - mean_squared_error: 0.52 - 0s 95us/step - loss: 0.0917 - mean_squared_error: 0.6218\n",
      "Epoch 62/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1116 - mean_squared_error: 0.58 - 0s 117us/step - loss: 0.0913 - mean_squared_error: 0.6179\n",
      "Epoch 63/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1595 - mean_squared_error: 0.75 - 0s 103us/step - loss: 0.0910 - mean_squared_error: 0.6148\n",
      "Epoch 64/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0980 - mean_squared_error: 0.46 - 0s 102us/step - loss: 0.0907 - mean_squared_error: 0.6122\n",
      "Epoch 65/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1523 - mean_squared_error: 0.63 - 0s 115us/step - loss: 0.0904 - mean_squared_error: 0.6094\n",
      "Epoch 66/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1289 - mean_squared_error: 0.79 - 0s 97us/step - loss: 0.0901 - mean_squared_error: 0.6071\n",
      "Epoch 67/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0787 - mean_squared_error: 0.56 - 0s 100us/step - loss: 0.0898 - mean_squared_error: 0.6041\n",
      "Epoch 68/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.42 - 0s 101us/step - loss: 0.0894 - mean_squared_error: 0.6016\n",
      "Epoch 69/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0928 - mean_squared_error: 0.59 - 0s 118us/step - loss: 0.0892 - mean_squared_error: 0.5992\n",
      "Epoch 70/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1589 - mean_squared_error: 0.81 - 0s 94us/step - loss: 0.0889 - mean_squared_error: 0.5967\n",
      "Epoch 71/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1217 - mean_squared_error: 0.69 - 0s 99us/step - loss: 0.0887 - mean_squared_error: 0.5949\n",
      "Epoch 72/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1022 - mean_squared_error: 0.46 - 0s 100us/step - loss: 0.0884 - mean_squared_error: 0.5919\n",
      "Epoch 73/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0733 - mean_squared_error: 0.51 - 0s 95us/step - loss: 0.0882 - mean_squared_error: 0.5905\n",
      "Epoch 74/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0809 - mean_squared_error: 0.53 - 0s 96us/step - loss: 0.0880 - mean_squared_error: 0.5888\n",
      "Epoch 75/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.37 - 0s 98us/step - loss: 0.0878 - mean_squared_error: 0.5869\n",
      "Epoch 76/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1180 - mean_squared_error: 0.73 - 0s 97us/step - loss: 0.0876 - mean_squared_error: 0.5852\n",
      "Epoch 77/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1160 - mean_squared_error: 0.75 - 0s 97us/step - loss: 0.0874 - mean_squared_error: 0.5838\n",
      "Epoch 78/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0838 - mean_squared_error: 0.57 - 0s 112us/step - loss: 0.0873 - mean_squared_error: 0.5822\n",
      "Epoch 79/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0786 - mean_squared_error: 0.52 - 0s 108us/step - loss: 0.0871 - mean_squared_error: 0.5808\n",
      "Epoch 80/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1044 - mean_squared_error: 0.60 - 0s 109us/step - loss: 0.0870 - mean_squared_error: 0.5797\n",
      "Epoch 81/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0872 - mean_squared_error: 0.56 - 0s 116us/step - loss: 0.0868 - mean_squared_error: 0.5781\n",
      "Epoch 82/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0721 - mean_squared_error: 0.64 - 0s 143us/step - loss: 0.0867 - mean_squared_error: 0.5771\n",
      "Epoch 83/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0819 - mean_squared_error: 0.67 - 0s 124us/step - loss: 0.0866 - mean_squared_error: 0.5757\n",
      "Epoch 84/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0912 - mean_squared_error: 0.52 - 0s 149us/step - loss: 0.0865 - mean_squared_error: 0.5743\n",
      "Epoch 85/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1158 - mean_squared_error: 0.86 - 0s 119us/step - loss: 0.0863 - mean_squared_error: 0.5731\n",
      "Epoch 86/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.53 - 0s 117us/step - loss: 0.0862 - mean_squared_error: 0.5720\n",
      "Epoch 87/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1080 - mean_squared_error: 0.65 - 0s 130us/step - loss: 0.0861 - mean_squared_error: 0.5712\n",
      "Epoch 88/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0603 - mean_squared_error: 0.43 - 0s 117us/step - loss: 0.0860 - mean_squared_error: 0.5700\n",
      "Epoch 89/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0744 - mean_squared_error: 0.60 - 0s 160us/step - loss: 0.0859 - mean_squared_error: 0.5691\n",
      "Epoch 90/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0540 - mean_squared_error: 0.50 - 0s 113us/step - loss: 0.0858 - mean_squared_error: 0.5684\n",
      "Epoch 91/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0494 - mean_squared_error: 0.57 - 0s 117us/step - loss: 0.0857 - mean_squared_error: 0.5676\n",
      "Epoch 92/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0450 - mean_squared_error: 0.38 - 0s 144us/step - loss: 0.0857 - mean_squared_error: 0.5670\n",
      "Epoch 93/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0645 - mean_squared_error: 0.44 - 0s 118us/step - loss: 0.0856 - mean_squared_error: 0.5665\n",
      "Epoch 94/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0705 - mean_squared_error: 0.55 - 0s 137us/step - loss: 0.0855 - mean_squared_error: 0.5655\n",
      "Epoch 95/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0563 - mean_squared_error: 0.52 - 0s 127us/step - loss: 0.0854 - mean_squared_error: 0.5646\n",
      "Epoch 96/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1010 - mean_squared_error: 0.59 - 0s 112us/step - loss: 0.0853 - mean_squared_error: 0.5636\n",
      "Epoch 97/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0605 - mean_squared_error: 0.50 - 0s 104us/step - loss: 0.0852 - mean_squared_error: 0.5632\n",
      "Epoch 98/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0841 - mean_squared_error: 0.60 - 0s 105us/step - loss: 0.0851 - mean_squared_error: 0.5625\n",
      "Epoch 99/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - ETA: 0s - loss: 0.1378 - mean_squared_error: 0.71 - 0s 101us/step - loss: 0.0850 - mean_squared_error: 0.5621\n",
      "Epoch 100/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1041 - mean_squared_error: 0.74 - 0s 95us/step - loss: 0.0850 - mean_squared_error: 0.5616\n",
      "Epoch 101/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0733 - mean_squared_error: 0.53 - 0s 99us/step - loss: 0.0849 - mean_squared_error: 0.5611\n",
      "Epoch 102/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.44 - 0s 101us/step - loss: 0.0848 - mean_squared_error: 0.5605\n",
      "Epoch 103/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1014 - mean_squared_error: 0.53 - 0s 103us/step - loss: 0.0848 - mean_squared_error: 0.5600\n",
      "Epoch 104/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0523 - mean_squared_error: 0.45 - 0s 95us/step - loss: 0.0847 - mean_squared_error: 0.5594\n",
      "Epoch 105/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0795 - mean_squared_error: 0.63 - 0s 99us/step - loss: 0.0847 - mean_squared_error: 0.5591\n",
      "Epoch 106/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1355 - mean_squared_error: 0.70 - 0s 116us/step - loss: 0.0846 - mean_squared_error: 0.5588\n",
      "Epoch 107/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1136 - mean_squared_error: 0.68 - 0s 100us/step - loss: 0.0846 - mean_squared_error: 0.5586\n",
      "Epoch 108/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0979 - mean_squared_error: 0.63 - 0s 99us/step - loss: 0.0845 - mean_squared_error: 0.5578\n",
      "Epoch 109/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0752 - mean_squared_error: 0.68 - 0s 110us/step - loss: 0.0845 - mean_squared_error: 0.5576\n",
      "Epoch 110/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0771 - mean_squared_error: 0.44 - 0s 95us/step - loss: 0.0844 - mean_squared_error: 0.5573\n",
      "Epoch 111/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0909 - mean_squared_error: 0.61 - 0s 103us/step - loss: 0.0844 - mean_squared_error: 0.5567\n",
      "Epoch 112/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1071 - mean_squared_error: 0.57 - 0s 93us/step - loss: 0.0843 - mean_squared_error: 0.5561\n",
      "Epoch 113/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0515 - mean_squared_error: 0.40 - 0s 100us/step - loss: 0.0842 - mean_squared_error: 0.5557\n",
      "Epoch 114/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0839 - mean_squared_error: 0.62 - 0s 97us/step - loss: 0.0841 - mean_squared_error: 0.5551\n",
      "Epoch 115/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0637 - mean_squared_error: 0.39 - 0s 94us/step - loss: 0.0840 - mean_squared_error: 0.5548\n",
      "Epoch 116/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0577 - mean_squared_error: 0.39 - 0s 96us/step - loss: 0.0839 - mean_squared_error: 0.5541\n",
      "Epoch 117/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0815 - mean_squared_error: 0.59 - 0s 97us/step - loss: 0.0837 - mean_squared_error: 0.5535\n",
      "Epoch 118/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1032 - mean_squared_error: 0.50 - 0s 94us/step - loss: 0.0837 - mean_squared_error: 0.5530\n",
      "Epoch 119/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.38 - 0s 105us/step - loss: 0.0836 - mean_squared_error: 0.5525\n",
      "Epoch 120/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0812 - mean_squared_error: 0.62 - 0s 99us/step - loss: 0.0834 - mean_squared_error: 0.5519\n",
      "Epoch 121/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0859 - mean_squared_error: 0.50 - 0s 103us/step - loss: 0.0834 - mean_squared_error: 0.5513\n",
      "Epoch 122/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0858 - mean_squared_error: 0.52 - 0s 99us/step - loss: 0.0833 - mean_squared_error: 0.5506\n",
      "Epoch 123/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1292 - mean_squared_error: 0.88 - 0s 93us/step - loss: 0.0832 - mean_squared_error: 0.5505\n",
      "Epoch 124/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.47 - 0s 99us/step - loss: 0.0831 - mean_squared_error: 0.5499\n",
      "Epoch 125/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0632 - mean_squared_error: 0.35 - 0s 109us/step - loss: 0.0830 - mean_squared_error: 0.5494\n",
      "Epoch 126/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0472 - mean_squared_error: 0.35 - 0s 103us/step - loss: 0.0830 - mean_squared_error: 0.5489\n",
      "Epoch 127/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0936 - mean_squared_error: 0.57 - 0s 94us/step - loss: 0.0829 - mean_squared_error: 0.5485\n",
      "Epoch 128/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.60 - 0s 97us/step - loss: 0.0829 - mean_squared_error: 0.5482\n",
      "Epoch 129/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1008 - mean_squared_error: 0.63 - 0s 101us/step - loss: 0.0827 - mean_squared_error: 0.5475\n",
      "Epoch 130/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0870 - mean_squared_error: 0.43 - 0s 105us/step - loss: 0.0827 - mean_squared_error: 0.5473\n",
      "Epoch 131/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0698 - mean_squared_error: 0.52 - 0s 99us/step - loss: 0.0827 - mean_squared_error: 0.5470\n",
      "Epoch 132/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0472 - mean_squared_error: 0.50 - 0s 98us/step - loss: 0.0826 - mean_squared_error: 0.5466\n",
      "Epoch 133/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1200 - mean_squared_error: 0.66 - 0s 111us/step - loss: 0.0826 - mean_squared_error: 0.5466\n",
      "Epoch 134/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.44 - 0s 117us/step - loss: 0.0825 - mean_squared_error: 0.5461\n",
      "Epoch 135/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0478 - mean_squared_error: 0.44 - 0s 99us/step - loss: 0.0825 - mean_squared_error: 0.5457\n",
      "Epoch 136/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0884 - mean_squared_error: 0.59 - 0s 97us/step - loss: 0.0824 - mean_squared_error: 0.5458\n",
      "Epoch 137/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1082 - mean_squared_error: 0.56 - 0s 99us/step - loss: 0.0824 - mean_squared_error: 0.5456\n",
      "Epoch 138/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0603 - mean_squared_error: 0.61 - 0s 95us/step - loss: 0.0823 - mean_squared_error: 0.5451\n",
      "Epoch 139/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1005 - mean_squared_error: 0.59 - 0s 98us/step - loss: 0.0823 - mean_squared_error: 0.5451\n",
      "Epoch 140/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0692 - mean_squared_error: 0.41 - 0s 97us/step - loss: 0.0823 - mean_squared_error: 0.5448\n",
      "Epoch 141/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1134 - mean_squared_error: 0.60 - 0s 94us/step - loss: 0.0823 - mean_squared_error: 0.5444\n",
      "Epoch 142/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0800 - mean_squared_error: 0.54 - 0s 101us/step - loss: 0.0822 - mean_squared_error: 0.5442\n",
      "Epoch 143/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0576 - mean_squared_error: 0.40 - 0s 96us/step - loss: 0.0822 - mean_squared_error: 0.5440\n",
      "Epoch 144/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1224 - mean_squared_error: 0.57 - 0s 103us/step - loss: 0.0821 - mean_squared_error: 0.5438\n",
      "Epoch 145/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0848 - mean_squared_error: 0.50 - 0s 91us/step - loss: 0.0821 - mean_squared_error: 0.5435\n",
      "Epoch 146/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0558 - mean_squared_error: 0.43 - 0s 118us/step - loss: 0.0821 - mean_squared_error: 0.5433\n",
      "Epoch 147/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0866 - mean_squared_error: 0.55 - 0s 100us/step - loss: 0.0820 - mean_squared_error: 0.5432\n",
      "Epoch 148/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.33 - 0s 101us/step - loss: 0.0820 - mean_squared_error: 0.5429\n",
      "Epoch 149/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0744 - mean_squared_error: 0.54 - 0s 98us/step - loss: 0.0820 - mean_squared_error: 0.5427\n",
      "Epoch 150/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0813 - mean_squared_error: 0.63 - 0s 99us/step - loss: 0.0819 - mean_squared_error: 0.5424\n",
      "Epoch 151/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1693 - mean_squared_error: 0.87 - 0s 95us/step - loss: 0.0819 - mean_squared_error: 0.5424\n",
      "Epoch 152/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.36 - 0s 96us/step - loss: 0.0819 - mean_squared_error: 0.5418\n",
      "Epoch 153/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1064 - mean_squared_error: 0.58 - 0s 95us/step - loss: 0.0819 - mean_squared_error: 0.5418\n",
      "Epoch 154/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0702 - mean_squared_error: 0.43 - 0s 93us/step - loss: 0.0819 - mean_squared_error: 0.5415\n",
      "Epoch 155/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0839 - mean_squared_error: 0.57 - 0s 114us/step - loss: 0.0819 - mean_squared_error: 0.5415\n",
      "Epoch 156/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1048 - mean_squared_error: 0.65 - 0s 90us/step - loss: 0.0818 - mean_squared_error: 0.5415\n",
      "Epoch 157/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0982 - mean_squared_error: 0.52 - 0s 117us/step - loss: 0.0818 - mean_squared_error: 0.5412\n",
      "Epoch 158/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0612 - mean_squared_error: 0.38 - 0s 95us/step - loss: 0.0817 - mean_squared_error: 0.5411\n",
      "Epoch 159/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0847 - mean_squared_error: 0.40 - 0s 101us/step - loss: 0.0817 - mean_squared_error: 0.5409\n",
      "Epoch 160/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.37 - 0s 112us/step - loss: 0.0817 - mean_squared_error: 0.5406\n",
      "Epoch 161/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0841 - mean_squared_error: 0.61 - 0s 99us/step - loss: 0.0816 - mean_squared_error: 0.5401\n",
      "Epoch 162/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0647 - mean_squared_error: 0.41 - 0s 125us/step - loss: 0.0816 - mean_squared_error: 0.5401\n",
      "Epoch 163/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1028 - mean_squared_error: 0.65 - 0s 95us/step - loss: 0.0816 - mean_squared_error: 0.5402\n",
      "Epoch 164/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0894 - mean_squared_error: 0.50 - 0s 96us/step - loss: 0.0816 - mean_squared_error: 0.5398\n",
      "Epoch 165/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0946 - mean_squared_error: 0.71 - 0s 95us/step - loss: 0.0815 - mean_squared_error: 0.5393\n",
      "Epoch 166/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1093 - mean_squared_error: 0.68 - 0s 106us/step - loss: 0.0816 - mean_squared_error: 0.5392\n",
      "Epoch 167/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0522 - mean_squared_error: 0.38 - 0s 104us/step - loss: 0.0816 - mean_squared_error: 0.5392\n",
      "Epoch 168/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0744 - mean_squared_error: 0.58 - 0s 95us/step - loss: 0.0815 - mean_squared_error: 0.5391\n",
      "Epoch 169/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1150 - mean_squared_error: 0.63 - 0s 115us/step - loss: 0.0815 - mean_squared_error: 0.5387\n",
      "Epoch 170/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1145 - mean_squared_error: 0.62 - 0s 109us/step - loss: 0.0814 - mean_squared_error: 0.5386\n",
      "Epoch 171/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1393 - mean_squared_error: 0.79 - 0s 118us/step - loss: 0.0814 - mean_squared_error: 0.5384\n",
      "Epoch 172/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0875 - mean_squared_error: 0.46 - 0s 124us/step - loss: 0.0814 - mean_squared_error: 0.5383\n",
      "Epoch 173/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1038 - mean_squared_error: 0.56 - 0s 97us/step - loss: 0.0814 - mean_squared_error: 0.5381\n",
      "Epoch 174/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0803 - mean_squared_error: 0.56 - 0s 95us/step - loss: 0.0814 - mean_squared_error: 0.5379\n",
      "Epoch 175/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0821 - mean_squared_error: 0.50 - 0s 96us/step - loss: 0.0814 - mean_squared_error: 0.5377\n",
      "Epoch 176/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0893 - mean_squared_error: 0.64 - 0s 101us/step - loss: 0.0813 - mean_squared_error: 0.5374\n",
      "Epoch 177/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0706 - mean_squared_error: 0.55 - 0s 103us/step - loss: 0.0813 - mean_squared_error: 0.5371\n",
      "Epoch 178/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1293 - mean_squared_error: 0.74 - 0s 97us/step - loss: 0.0813 - mean_squared_error: 0.5369\n",
      "Epoch 179/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.47 - 0s 111us/step - loss: 0.0813 - mean_squared_error: 0.5367\n",
      "Epoch 180/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1171 - mean_squared_error: 0.56 - 0s 97us/step - loss: 0.0812 - mean_squared_error: 0.5367\n",
      "Epoch 181/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1062 - mean_squared_error: 0.57 - 0s 94us/step - loss: 0.0812 - mean_squared_error: 0.5366\n",
      "Epoch 182/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0569 - mean_squared_error: 0.50 - 0s 107us/step - loss: 0.0812 - mean_squared_error: 0.5363\n",
      "Epoch 183/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0839 - mean_squared_error: 0.51 - 0s 130us/step - loss: 0.0812 - mean_squared_error: 0.5361\n",
      "Epoch 184/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.37 - 0s 103us/step - loss: 0.0812 - mean_squared_error: 0.5360\n",
      "Epoch 185/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0566 - mean_squared_error: 0.42 - 0s 104us/step - loss: 0.0812 - mean_squared_error: 0.5359\n",
      "Epoch 186/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1479 - mean_squared_error: 0.83 - 0s 98us/step - loss: 0.0812 - mean_squared_error: 0.5361\n",
      "Epoch 187/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.36 - 0s 92us/step - loss: 0.0811 - mean_squared_error: 0.5357\n",
      "Epoch 188/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0762 - mean_squared_error: 0.50 - 0s 97us/step - loss: 0.0811 - mean_squared_error: 0.5358\n",
      "Epoch 189/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0955 - mean_squared_error: 0.49 - 0s 99us/step - loss: 0.0811 - mean_squared_error: 0.5356\n",
      "Epoch 190/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0584 - mean_squared_error: 0.52 - 0s 111us/step - loss: 0.0811 - mean_squared_error: 0.5350\n",
      "Epoch 191/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0790 - mean_squared_error: 0.55 - 0s 115us/step - loss: 0.0811 - mean_squared_error: 0.5346\n",
      "Epoch 192/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.48 - 0s 120us/step - loss: 0.0810 - mean_squared_error: 0.5346\n",
      "Epoch 193/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0492 - mean_squared_error: 0.44 - 0s 96us/step - loss: 0.0810 - mean_squared_error: 0.5342\n",
      "Epoch 194/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0584 - mean_squared_error: 0.36 - 0s 99us/step - loss: 0.0811 - mean_squared_error: 0.5340\n",
      "Epoch 195/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0974 - mean_squared_error: 0.64 - 0s 100us/step - loss: 0.0810 - mean_squared_error: 0.5342\n",
      "Epoch 196/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1122 - mean_squared_error: 0.63 - 0s 113us/step - loss: 0.0810 - mean_squared_error: 0.5340\n",
      "Epoch 197/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - ETA: 0s - loss: 0.0612 - mean_squared_error: 0.44 - 0s 105us/step - loss: 0.0809 - mean_squared_error: 0.5337\n",
      "Epoch 198/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0566 - mean_squared_error: 0.48 - 0s 94us/step - loss: 0.0809 - mean_squared_error: 0.5335\n",
      "Epoch 199/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0844 - mean_squared_error: 0.58 - 0s 92us/step - loss: 0.0809 - mean_squared_error: 0.5334\n",
      "Epoch 200/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.48 - 0s 98us/step - loss: 0.0809 - mean_squared_error: 0.5333\n",
      "Epoch 201/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0800 - mean_squared_error: 0.48 - 0s 99us/step - loss: 0.0809 - mean_squared_error: 0.5331\n",
      "Epoch 202/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0828 - mean_squared_error: 0.63 - 0s 120us/step - loss: 0.0809 - mean_squared_error: 0.5330\n",
      "Epoch 203/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.41 - 0s 96us/step - loss: 0.0809 - mean_squared_error: 0.5327\n",
      "Epoch 204/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0757 - mean_squared_error: 0.55 - 0s 90us/step - loss: 0.0809 - mean_squared_error: 0.5328\n",
      "Epoch 205/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0538 - mean_squared_error: 0.38 - 0s 94us/step - loss: 0.0808 - mean_squared_error: 0.5331\n",
      "Epoch 206/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0546 - mean_squared_error: 0.31 - 0s 102us/step - loss: 0.0808 - mean_squared_error: 0.5331\n",
      "Epoch 207/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.43 - 0s 97us/step - loss: 0.0808 - mean_squared_error: 0.5329\n",
      "Epoch 208/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0531 - mean_squared_error: 0.52 - 0s 100us/step - loss: 0.0808 - mean_squared_error: 0.5327\n",
      "Epoch 209/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0791 - mean_squared_error: 0.56 - 0s 105us/step - loss: 0.0808 - mean_squared_error: 0.5325\n",
      "Epoch 210/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0752 - mean_squared_error: 0.50 - 0s 104us/step - loss: 0.0808 - mean_squared_error: 0.5322\n",
      "Epoch 211/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0917 - mean_squared_error: 0.50 - 0s 109us/step - loss: 0.0807 - mean_squared_error: 0.5319\n",
      "Epoch 212/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0570 - mean_squared_error: 0.46 - 0s 133us/step - loss: 0.0807 - mean_squared_error: 0.5319\n",
      "Epoch 213/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0862 - mean_squared_error: 0.65 - 0s 160us/step - loss: 0.0807 - mean_squared_error: 0.5318\n",
      "Epoch 214/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0734 - mean_squared_error: 0.60 - 0s 99us/step - loss: 0.0807 - mean_squared_error: 0.5318\n",
      "Epoch 215/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1053 - mean_squared_error: 0.53 - 0s 93us/step - loss: 0.0807 - mean_squared_error: 0.5315\n",
      "Epoch 216/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1079 - mean_squared_error: 0.71 - 0s 98us/step - loss: 0.0807 - mean_squared_error: 0.5312\n",
      "Epoch 217/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0921 - mean_squared_error: 0.67 - 0s 96us/step - loss: 0.0807 - mean_squared_error: 0.5310\n",
      "Epoch 218/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0992 - mean_squared_error: 0.59 - 0s 94us/step - loss: 0.0806 - mean_squared_error: 0.5309\n",
      "Epoch 219/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0906 - mean_squared_error: 0.57 - 0s 98us/step - loss: 0.0806 - mean_squared_error: 0.5305\n",
      "Epoch 220/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0889 - mean_squared_error: 0.56 - 0s 96us/step - loss: 0.0806 - mean_squared_error: 0.5308\n",
      "Epoch 221/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0642 - mean_squared_error: 0.51 - 0s 96us/step - loss: 0.0806 - mean_squared_error: 0.5304\n",
      "Epoch 222/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0573 - mean_squared_error: 0.44 - 0s 108us/step - loss: 0.0806 - mean_squared_error: 0.5303\n",
      "Epoch 223/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0897 - mean_squared_error: 0.60 - 0s 118us/step - loss: 0.0806 - mean_squared_error: 0.5303\n",
      "Epoch 224/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1109 - mean_squared_error: 0.55 - 0s 106us/step - loss: 0.0805 - mean_squared_error: 0.5302\n",
      "Epoch 225/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0965 - mean_squared_error: 0.65 - 0s 108us/step - loss: 0.0805 - mean_squared_error: 0.5304\n",
      "Epoch 226/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1009 - mean_squared_error: 0.55 - 0s 99us/step - loss: 0.0806 - mean_squared_error: 0.5301\n",
      "Epoch 227/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0722 - mean_squared_error: 0.45 - 0s 97us/step - loss: 0.0806 - mean_squared_error: 0.5306\n",
      "Epoch 228/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0519 - mean_squared_error: 0.42 - 0s 96us/step - loss: 0.0805 - mean_squared_error: 0.5300\n",
      "Epoch 229/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0763 - mean_squared_error: 0.50 - 0s 99us/step - loss: 0.0805 - mean_squared_error: 0.5299\n",
      "Epoch 230/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0871 - mean_squared_error: 0.54 - 0s 99us/step - loss: 0.0805 - mean_squared_error: 0.5295\n",
      "Epoch 231/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0853 - mean_squared_error: 0.66 - 0s 92us/step - loss: 0.0805 - mean_squared_error: 0.5296\n",
      "Epoch 232/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0883 - mean_squared_error: 0.60 - 0s 95us/step - loss: 0.0805 - mean_squared_error: 0.5294\n",
      "Epoch 233/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0729 - mean_squared_error: 0.52 - 0s 118us/step - loss: 0.0804 - mean_squared_error: 0.5289\n",
      "Epoch 234/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0635 - mean_squared_error: 0.55 - 0s 92us/step - loss: 0.0804 - mean_squared_error: 0.5290\n",
      "Epoch 235/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0755 - mean_squared_error: 0.42 - 0s 98us/step - loss: 0.0805 - mean_squared_error: 0.5295\n",
      "Epoch 236/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1385 - mean_squared_error: 0.63 - 0s 94us/step - loss: 0.0804 - mean_squared_error: 0.5289\n",
      "Epoch 237/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0765 - mean_squared_error: 0.42 - 0s 95us/step - loss: 0.0804 - mean_squared_error: 0.5286\n",
      "Epoch 238/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0824 - mean_squared_error: 0.52 - 0s 97us/step - loss: 0.0804 - mean_squared_error: 0.5286\n",
      "Epoch 239/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0993 - mean_squared_error: 0.70 - 0s 99us/step - loss: 0.0804 - mean_squared_error: 0.5284\n",
      "Epoch 240/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0658 - mean_squared_error: 0.37 - 0s 97us/step - loss: 0.0803 - mean_squared_error: 0.5282\n",
      "Epoch 241/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0752 - mean_squared_error: 0.49 - 0s 109us/step - loss: 0.0803 - mean_squared_error: 0.5279\n",
      "Epoch 242/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0642 - mean_squared_error: 0.38 - 0s 101us/step - loss: 0.0804 - mean_squared_error: 0.5277\n",
      "Epoch 243/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1114 - mean_squared_error: 0.60 - 0s 103us/step - loss: 0.0804 - mean_squared_error: 0.5279\n",
      "Epoch 244/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0570 - mean_squared_error: 0.42 - 0s 94us/step - loss: 0.0803 - mean_squared_error: 0.5276\n",
      "Epoch 245/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1058 - mean_squared_error: 0.51 - 0s 106us/step - loss: 0.0803 - mean_squared_error: 0.5275\n",
      "Epoch 246/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0983 - mean_squared_error: 0.53 - 0s 109us/step - loss: 0.0803 - mean_squared_error: 0.5274\n",
      "Epoch 247/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0797 - mean_squared_error: 0.44 - 0s 101us/step - loss: 0.0803 - mean_squared_error: 0.5270\n",
      "Epoch 248/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.51 - 0s 109us/step - loss: 0.0802 - mean_squared_error: 0.5270\n",
      "Epoch 249/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1705 - mean_squared_error: 0.80 - 0s 102us/step - loss: 0.0802 - mean_squared_error: 0.5269\n",
      "Epoch 250/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1124 - mean_squared_error: 0.65 - 0s 107us/step - loss: 0.0802 - mean_squared_error: 0.5268\n",
      "Epoch 251/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.48 - 0s 95us/step - loss: 0.0802 - mean_squared_error: 0.5269\n",
      "Epoch 252/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0626 - mean_squared_error: 0.45 - 0s 95us/step - loss: 0.0802 - mean_squared_error: 0.5268\n",
      "Epoch 253/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0792 - mean_squared_error: 0.49 - 0s 93us/step - loss: 0.0802 - mean_squared_error: 0.5265\n",
      "Epoch 254/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0711 - mean_squared_error: 0.57 - 0s 104us/step - loss: 0.0802 - mean_squared_error: 0.5264\n",
      "Epoch 255/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1009 - mean_squared_error: 0.65 - 0s 101us/step - loss: 0.0801 - mean_squared_error: 0.5264\n",
      "Epoch 256/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0831 - mean_squared_error: 0.48 - 0s 97us/step - loss: 0.0801 - mean_squared_error: 0.5265\n",
      "Epoch 257/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0897 - mean_squared_error: 0.49 - 0s 102us/step - loss: 0.0802 - mean_squared_error: 0.5264\n",
      "Epoch 258/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0527 - mean_squared_error: 0.36 - 0s 97us/step - loss: 0.0801 - mean_squared_error: 0.5261\n",
      "Epoch 259/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0619 - mean_squared_error: 0.58 - 0s 92us/step - loss: 0.0801 - mean_squared_error: 0.5261\n",
      "Epoch 260/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1106 - mean_squared_error: 0.51 - 0s 100us/step - loss: 0.0801 - mean_squared_error: 0.5259\n",
      "Epoch 261/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0668 - mean_squared_error: 0.38 - 0s 97us/step - loss: 0.0801 - mean_squared_error: 0.5259\n",
      "Epoch 262/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0846 - mean_squared_error: 0.51 - 0s 96us/step - loss: 0.0801 - mean_squared_error: 0.5258\n",
      "Epoch 263/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0602 - mean_squared_error: 0.44 - 0s 96us/step - loss: 0.0801 - mean_squared_error: 0.5256\n",
      "Epoch 264/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0716 - mean_squared_error: 0.49 - 0s 96us/step - loss: 0.0800 - mean_squared_error: 0.5255\n",
      "Epoch 265/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0748 - mean_squared_error: 0.55 - 0s 93us/step - loss: 0.0800 - mean_squared_error: 0.5253\n",
      "Epoch 266/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0801 - mean_squared_error: 0.45 - 0s 93us/step - loss: 0.0800 - mean_squared_error: 0.5253\n",
      "Epoch 267/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0782 - mean_squared_error: 0.59 - 0s 98us/step - loss: 0.0801 - mean_squared_error: 0.5252\n",
      "Epoch 268/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0956 - mean_squared_error: 0.57 - 0s 107us/step - loss: 0.0800 - mean_squared_error: 0.5247\n",
      "Epoch 269/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0804 - mean_squared_error: 0.55 - 0s 101us/step - loss: 0.0800 - mean_squared_error: 0.5244\n",
      "Epoch 270/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0950 - mean_squared_error: 0.61 - 0s 101us/step - loss: 0.0800 - mean_squared_error: 0.5244\n",
      "Epoch 271/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0761 - mean_squared_error: 0.41 - 0s 111us/step - loss: 0.0800 - mean_squared_error: 0.5244\n",
      "Epoch 272/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0547 - mean_squared_error: 0.40 - 0s 102us/step - loss: 0.0800 - mean_squared_error: 0.5245\n",
      "Epoch 273/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0794 - mean_squared_error: 0.44 - 0s 137us/step - loss: 0.0800 - mean_squared_error: 0.5245\n",
      "Epoch 274/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0497 - mean_squared_error: 0.47 - 0s 121us/step - loss: 0.0800 - mean_squared_error: 0.5243\n",
      "Epoch 275/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0740 - mean_squared_error: 0.54 - 0s 114us/step - loss: 0.0800 - mean_squared_error: 0.5243\n",
      "Epoch 276/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1027 - mean_squared_error: 0.63 - 0s 103us/step - loss: 0.0799 - mean_squared_error: 0.5239\n",
      "Epoch 277/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0556 - mean_squared_error: 0.46 - 0s 117us/step - loss: 0.0799 - mean_squared_error: 0.5237\n",
      "Epoch 278/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0946 - mean_squared_error: 0.60 - 0s 106us/step - loss: 0.0799 - mean_squared_error: 0.5236\n",
      "Epoch 279/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0673 - mean_squared_error: 0.59 - 0s 132us/step - loss: 0.0799 - mean_squared_error: 0.5237\n",
      "Epoch 280/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0509 - mean_squared_error: 0.43 - 0s 126us/step - loss: 0.0799 - mean_squared_error: 0.5235\n",
      "Epoch 281/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0772 - mean_squared_error: 0.57 - 0s 114us/step - loss: 0.0799 - mean_squared_error: 0.5234\n",
      "Epoch 282/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1148 - mean_squared_error: 0.64 - 0s 110us/step - loss: 0.0799 - mean_squared_error: 0.5235\n",
      "Epoch 283/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0534 - mean_squared_error: 0.52 - 0s 120us/step - loss: 0.0798 - mean_squared_error: 0.5233\n",
      "Epoch 284/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0751 - mean_squared_error: 0.52 - 0s 111us/step - loss: 0.0798 - mean_squared_error: 0.5234\n",
      "Epoch 285/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0927 - mean_squared_error: 0.52 - 0s 123us/step - loss: 0.0799 - mean_squared_error: 0.5235\n",
      "Epoch 286/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.39 - 0s 115us/step - loss: 0.0798 - mean_squared_error: 0.5232\n",
      "Epoch 287/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0590 - mean_squared_error: 0.32 - 0s 113us/step - loss: 0.0798 - mean_squared_error: 0.5233\n",
      "Epoch 288/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.41 - 0s 111us/step - loss: 0.0797 - mean_squared_error: 0.5234\n",
      "Epoch 289/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0780 - mean_squared_error: 0.63 - 0s 113us/step - loss: 0.0797 - mean_squared_error: 0.5238\n",
      "Epoch 290/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0539 - mean_squared_error: 0.56 - 0s 122us/step - loss: 0.0797 - mean_squared_error: 0.5236\n",
      "Epoch 291/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0648 - mean_squared_error: 0.43 - 0s 121us/step - loss: 0.0797 - mean_squared_error: 0.5236\n",
      "Epoch 292/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0788 - mean_squared_error: 0.68 - 0s 126us/step - loss: 0.0796 - mean_squared_error: 0.5235\n",
      "Epoch 293/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0510 - mean_squared_error: 0.53 - 0s 106us/step - loss: 0.0796 - mean_squared_error: 0.5232\n",
      "Epoch 294/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0486 - mean_squared_error: 0.44 - 0s 114us/step - loss: 0.0796 - mean_squared_error: 0.5233\n",
      "Epoch 295/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - ETA: 0s - loss: 0.1138 - mean_squared_error: 0.74 - 0s 113us/step - loss: 0.0795 - mean_squared_error: 0.5231\n",
      "Epoch 296/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0582 - mean_squared_error: 0.51 - 0s 111us/step - loss: 0.0795 - mean_squared_error: 0.5236\n",
      "Epoch 297/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0881 - mean_squared_error: 0.45 - 0s 122us/step - loss: 0.0794 - mean_squared_error: 0.5231\n",
      "Epoch 298/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0816 - mean_squared_error: 0.57 - 0s 162us/step - loss: 0.0794 - mean_squared_error: 0.5230\n",
      "Epoch 299/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0884 - mean_squared_error: 0.49 - 0s 139us/step - loss: 0.0793 - mean_squared_error: 0.5227\n",
      "Epoch 300/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0667 - mean_squared_error: 0.57 - 0s 145us/step - loss: 0.0792 - mean_squared_error: 0.5223\n",
      "Epoch 301/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.52 - 0s 143us/step - loss: 0.0791 - mean_squared_error: 0.5220\n",
      "Epoch 302/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1086 - mean_squared_error: 0.71 - 0s 133us/step - loss: 0.0791 - mean_squared_error: 0.5222\n",
      "Epoch 303/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0941 - mean_squared_error: 0.48 - 0s 161us/step - loss: 0.0790 - mean_squared_error: 0.5224\n",
      "Epoch 304/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.45 - 0s 218us/step - loss: 0.0790 - mean_squared_error: 0.5225\n",
      "Epoch 305/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0948 - mean_squared_error: 0.72 - 0s 156us/step - loss: 0.0790 - mean_squared_error: 0.5223\n",
      "Epoch 306/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.35 - 0s 159us/step - loss: 0.0789 - mean_squared_error: 0.5220\n",
      "Epoch 307/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0866 - mean_squared_error: 0.49 - 0s 141us/step - loss: 0.0789 - mean_squared_error: 0.5218\n",
      "Epoch 308/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.50 - 0s 136us/step - loss: 0.0789 - mean_squared_error: 0.5221\n",
      "Epoch 309/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.46 - 0s 129us/step - loss: 0.0789 - mean_squared_error: 0.5223\n",
      "Epoch 310/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0857 - mean_squared_error: 0.48 - 0s 118us/step - loss: 0.0788 - mean_squared_error: 0.5220\n",
      "Epoch 311/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0531 - mean_squared_error: 0.35 - 0s 144us/step - loss: 0.0788 - mean_squared_error: 0.5219\n",
      "Epoch 312/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0623 - mean_squared_error: 0.44 - 0s 122us/step - loss: 0.0788 - mean_squared_error: 0.5217\n",
      "Epoch 313/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0798 - mean_squared_error: 0.51 - 0s 99us/step - loss: 0.0787 - mean_squared_error: 0.5215\n",
      "Epoch 314/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0680 - mean_squared_error: 0.38 - 0s 120us/step - loss: 0.0787 - mean_squared_error: 0.5215\n",
      "Epoch 315/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0876 - mean_squared_error: 0.48 - 0s 109us/step - loss: 0.0786 - mean_squared_error: 0.5215\n",
      "Epoch 316/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.40 - 0s 108us/step - loss: 0.0786 - mean_squared_error: 0.5215\n",
      "Epoch 317/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0959 - mean_squared_error: 0.68 - 0s 140us/step - loss: 0.0785 - mean_squared_error: 0.5213\n",
      "Epoch 318/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0746 - mean_squared_error: 0.46 - 0s 125us/step - loss: 0.0785 - mean_squared_error: 0.5213\n",
      "Epoch 319/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1132 - mean_squared_error: 0.73 - 0s 110us/step - loss: 0.0784 - mean_squared_error: 0.5217\n",
      "Epoch 320/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0877 - mean_squared_error: 0.65 - 0s 113us/step - loss: 0.0784 - mean_squared_error: 0.5223\n",
      "Epoch 321/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0694 - mean_squared_error: 0.52 - 0s 109us/step - loss: 0.0783 - mean_squared_error: 0.5221\n",
      "Epoch 322/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1033 - mean_squared_error: 0.44 - 0s 107us/step - loss: 0.0783 - mean_squared_error: 0.5224\n",
      "Epoch 323/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0729 - mean_squared_error: 0.39 - 0s 110us/step - loss: 0.0782 - mean_squared_error: 0.5228\n",
      "Epoch 324/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0614 - mean_squared_error: 0.43 - 0s 118us/step - loss: 0.0782 - mean_squared_error: 0.5227\n",
      "Epoch 325/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0583 - mean_squared_error: 0.46 - 0s 120us/step - loss: 0.0781 - mean_squared_error: 0.5229\n",
      "Epoch 326/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0546 - mean_squared_error: 0.53 - 0s 114us/step - loss: 0.0782 - mean_squared_error: 0.5230\n",
      "Epoch 327/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0757 - mean_squared_error: 0.57 - 0s 119us/step - loss: 0.0781 - mean_squared_error: 0.5229\n",
      "Epoch 328/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0655 - mean_squared_error: 0.49 - 0s 108us/step - loss: 0.0781 - mean_squared_error: 0.5229\n",
      "Epoch 329/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0733 - mean_squared_error: 0.44 - 0s 118us/step - loss: 0.0781 - mean_squared_error: 0.5234\n",
      "Epoch 330/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.46 - 0s 106us/step - loss: 0.0780 - mean_squared_error: 0.5239\n",
      "Epoch 331/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0594 - mean_squared_error: 0.48 - 0s 108us/step - loss: 0.0780 - mean_squared_error: 0.5244\n",
      "Epoch 332/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0807 - mean_squared_error: 0.48 - 0s 122us/step - loss: 0.0780 - mean_squared_error: 0.5251\n",
      "Epoch 333/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1024 - mean_squared_error: 0.57 - 0s 115us/step - loss: 0.0780 - mean_squared_error: 0.5252\n",
      "Epoch 334/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.34 - 0s 113us/step - loss: 0.0779 - mean_squared_error: 0.5247\n",
      "Epoch 335/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0594 - mean_squared_error: 0.54 - 0s 112us/step - loss: 0.0780 - mean_squared_error: 0.5250\n",
      "Epoch 336/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1109 - mean_squared_error: 0.54 - 0s 126us/step - loss: 0.0779 - mean_squared_error: 0.5250\n",
      "Epoch 337/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0864 - mean_squared_error: 0.43 - 0s 94us/step - loss: 0.0779 - mean_squared_error: 0.5254\n",
      "Epoch 338/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0761 - mean_squared_error: 0.45 - 0s 91us/step - loss: 0.0779 - mean_squared_error: 0.5253\n",
      "Epoch 339/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1310 - mean_squared_error: 0.66 - 0s 100us/step - loss: 0.0779 - mean_squared_error: 0.5257\n",
      "Epoch 340/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.42 - 0s 98us/step - loss: 0.0778 - mean_squared_error: 0.5259\n",
      "Epoch 341/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0656 - mean_squared_error: 0.59 - 0s 99us/step - loss: 0.0778 - mean_squared_error: 0.5260\n",
      "Epoch 342/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0881 - mean_squared_error: 0.50 - 0s 120us/step - loss: 0.0778 - mean_squared_error: 0.5257\n",
      "Epoch 343/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0890 - mean_squared_error: 0.67 - 0s 96us/step - loss: 0.0778 - mean_squared_error: 0.5255\n",
      "Epoch 344/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0808 - mean_squared_error: 0.57 - 0s 112us/step - loss: 0.0777 - mean_squared_error: 0.5255\n",
      "Epoch 345/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0566 - mean_squared_error: 0.44 - 0s 138us/step - loss: 0.0777 - mean_squared_error: 0.5257\n",
      "Epoch 346/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1251 - mean_squared_error: 0.71 - 0s 115us/step - loss: 0.0777 - mean_squared_error: 0.5257\n",
      "Epoch 347/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0587 - mean_squared_error: 0.44 - 0s 140us/step - loss: 0.0777 - mean_squared_error: 0.5257\n",
      "Epoch 348/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1138 - mean_squared_error: 0.59 - 0s 119us/step - loss: 0.0777 - mean_squared_error: 0.5258\n",
      "Epoch 349/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0467 - mean_squared_error: 0.51 - 0s 122us/step - loss: 0.0777 - mean_squared_error: 0.5261\n",
      "Epoch 350/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0786 - mean_squared_error: 0.65 - 0s 120us/step - loss: 0.0777 - mean_squared_error: 0.5264\n",
      "Epoch 351/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0577 - mean_squared_error: 0.44 - 0s 123us/step - loss: 0.0776 - mean_squared_error: 0.5264\n",
      "Epoch 352/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.31 - 0s 146us/step - loss: 0.0777 - mean_squared_error: 0.5266\n",
      "Epoch 353/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1317 - mean_squared_error: 0.89 - 0s 142us/step - loss: 0.0776 - mean_squared_error: 0.5265\n",
      "Epoch 354/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0893 - mean_squared_error: 0.57 - 0s 116us/step - loss: 0.0776 - mean_squared_error: 0.5267\n",
      "Epoch 355/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0839 - mean_squared_error: 0.54 - 0s 153us/step - loss: 0.0776 - mean_squared_error: 0.5265\n",
      "Epoch 356/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0591 - mean_squared_error: 0.41 - 0s 113us/step - loss: 0.0776 - mean_squared_error: 0.5262\n",
      "Epoch 357/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0898 - mean_squared_error: 0.53 - 0s 142us/step - loss: 0.0775 - mean_squared_error: 0.5260\n",
      "Epoch 358/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0706 - mean_squared_error: 0.44 - 0s 115us/step - loss: 0.0775 - mean_squared_error: 0.5262\n",
      "Epoch 359/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1270 - mean_squared_error: 0.70 - 0s 131us/step - loss: 0.0775 - mean_squared_error: 0.5261\n",
      "Epoch 360/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0615 - mean_squared_error: 0.35 - 0s 105us/step - loss: 0.0775 - mean_squared_error: 0.5265\n",
      "Epoch 361/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0606 - mean_squared_error: 0.43 - 0s 110us/step - loss: 0.0775 - mean_squared_error: 0.5263\n",
      "Epoch 362/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0807 - mean_squared_error: 0.54 - 0s 101us/step - loss: 0.0775 - mean_squared_error: 0.5262\n",
      "Epoch 363/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1107 - mean_squared_error: 0.62 - 0s 112us/step - loss: 0.0775 - mean_squared_error: 0.5265\n",
      "Epoch 364/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0673 - mean_squared_error: 0.51 - 0s 92us/step - loss: 0.0775 - mean_squared_error: 0.5271\n",
      "Epoch 365/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0559 - mean_squared_error: 0.36 - 0s 97us/step - loss: 0.0774 - mean_squared_error: 0.5271\n",
      "Epoch 366/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0570 - mean_squared_error: 0.47 - 0s 94us/step - loss: 0.0774 - mean_squared_error: 0.5269\n",
      "Epoch 367/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.53 - 0s 93us/step - loss: 0.0774 - mean_squared_error: 0.5268\n",
      "Epoch 368/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0699 - mean_squared_error: 0.41 - 0s 94us/step - loss: 0.0774 - mean_squared_error: 0.5267\n",
      "Epoch 369/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0624 - mean_squared_error: 0.60 - 0s 98us/step - loss: 0.0774 - mean_squared_error: 0.5265\n",
      "Epoch 370/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.41 - 0s 96us/step - loss: 0.0774 - mean_squared_error: 0.5267\n",
      "Epoch 371/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0789 - mean_squared_error: 0.62 - 0s 93us/step - loss: 0.0774 - mean_squared_error: 0.5267\n",
      "Epoch 372/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1026 - mean_squared_error: 0.65 - 0s 102us/step - loss: 0.0773 - mean_squared_error: 0.5263\n",
      "Epoch 373/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0627 - mean_squared_error: 0.53 - 0s 93us/step - loss: 0.0774 - mean_squared_error: 0.5267\n",
      "Epoch 374/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0700 - mean_squared_error: 0.53 - 0s 91us/step - loss: 0.0773 - mean_squared_error: 0.5267\n",
      "Epoch 375/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0546 - mean_squared_error: 0.40 - 0s 94us/step - loss: 0.0773 - mean_squared_error: 0.5270\n",
      "Epoch 376/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0639 - mean_squared_error: 0.49 - 0s 105us/step - loss: 0.0773 - mean_squared_error: 0.5270\n",
      "Epoch 377/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0995 - mean_squared_error: 0.72 - 0s 125us/step - loss: 0.0773 - mean_squared_error: 0.5266\n",
      "Epoch 378/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0936 - mean_squared_error: 0.60 - 0s 151us/step - loss: 0.0773 - mean_squared_error: 0.5273\n",
      "Epoch 379/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0669 - mean_squared_error: 0.56 - 0s 106us/step - loss: 0.0773 - mean_squared_error: 0.5274\n",
      "Epoch 380/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1135 - mean_squared_error: 0.66 - 0s 101us/step - loss: 0.0773 - mean_squared_error: 0.5275\n",
      "Epoch 381/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.33 - 0s 94us/step - loss: 0.0773 - mean_squared_error: 0.5270\n",
      "Epoch 382/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1160 - mean_squared_error: 0.69 - 0s 98us/step - loss: 0.0772 - mean_squared_error: 0.5268\n",
      "Epoch 383/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0736 - mean_squared_error: 0.56 - 0s 100us/step - loss: 0.0772 - mean_squared_error: 0.5270\n",
      "Epoch 384/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1019 - mean_squared_error: 0.58 - 0s 96us/step - loss: 0.0772 - mean_squared_error: 0.5270\n",
      "Epoch 385/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0517 - mean_squared_error: 0.41 - 0s 102us/step - loss: 0.0772 - mean_squared_error: 0.5268\n",
      "Epoch 386/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0854 - mean_squared_error: 0.50 - 0s 93us/step - loss: 0.0772 - mean_squared_error: 0.5270\n",
      "Epoch 387/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0554 - mean_squared_error: 0.47 - 0s 102us/step - loss: 0.0772 - mean_squared_error: 0.5273\n",
      "Epoch 388/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1049 - mean_squared_error: 0.61 - 0s 98us/step - loss: 0.0772 - mean_squared_error: 0.5277\n",
      "Epoch 389/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0724 - mean_squared_error: 0.52 - 0s 101us/step - loss: 0.0771 - mean_squared_error: 0.5279\n",
      "Epoch 390/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1184 - mean_squared_error: 0.56 - 0s 103us/step - loss: 0.0771 - mean_squared_error: 0.5279\n",
      "Epoch 391/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.46 - 0s 98us/step - loss: 0.0771 - mean_squared_error: 0.5279\n",
      "Epoch 392/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1160 - mean_squared_error: 0.68 - 0s 96us/step - loss: 0.0771 - mean_squared_error: 0.5277\n",
      "Epoch 393/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - ETA: 0s - loss: 0.1225 - mean_squared_error: 0.74 - 0s 105us/step - loss: 0.0771 - mean_squared_error: 0.5277\n",
      "Epoch 394/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0914 - mean_squared_error: 0.50 - 0s 103us/step - loss: 0.0771 - mean_squared_error: 0.5276\n",
      "Epoch 395/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0752 - mean_squared_error: 0.62 - 0s 93us/step - loss: 0.0771 - mean_squared_error: 0.5277\n",
      "Epoch 396/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1424 - mean_squared_error: 0.81 - 0s 94us/step - loss: 0.0771 - mean_squared_error: 0.5277\n",
      "Epoch 397/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0742 - mean_squared_error: 0.51 - 0s 99us/step - loss: 0.0771 - mean_squared_error: 0.5284\n",
      "Epoch 398/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0541 - mean_squared_error: 0.39 - 0s 104us/step - loss: 0.0771 - mean_squared_error: 0.5281\n",
      "Epoch 399/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0620 - mean_squared_error: 0.51 - 0s 100us/step - loss: 0.0771 - mean_squared_error: 0.5278\n",
      "Epoch 400/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0880 - mean_squared_error: 0.58 - 0s 97us/step - loss: 0.0771 - mean_squared_error: 0.5275\n",
      "Epoch 401/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1267 - mean_squared_error: 0.61 - 0s 99us/step - loss: 0.0771 - mean_squared_error: 0.5281\n",
      "Epoch 402/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0829 - mean_squared_error: 0.70 - 0s 97us/step - loss: 0.0770 - mean_squared_error: 0.5282\n",
      "Epoch 403/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0766 - mean_squared_error: 0.41 - 0s 98us/step - loss: 0.0770 - mean_squared_error: 0.5286\n",
      "Epoch 404/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1127 - mean_squared_error: 0.61 - 0s 95us/step - loss: 0.0769 - mean_squared_error: 0.5291\n",
      "Epoch 405/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.49 - 0s 109us/step - loss: 0.0768 - mean_squared_error: 0.5295\n",
      "Epoch 406/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0671 - mean_squared_error: 0.55 - 0s 95us/step - loss: 0.0768 - mean_squared_error: 0.5299\n",
      "Epoch 407/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1066 - mean_squared_error: 0.67 - 0s 99us/step - loss: 0.0767 - mean_squared_error: 0.5309\n",
      "Epoch 408/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0743 - mean_squared_error: 0.56 - 0s 102us/step - loss: 0.0766 - mean_squared_error: 0.5312\n",
      "Epoch 409/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0722 - mean_squared_error: 0.56 - 0s 100us/step - loss: 0.0766 - mean_squared_error: 0.5315\n",
      "Epoch 410/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0652 - mean_squared_error: 0.57 - 0s 99us/step - loss: 0.0766 - mean_squared_error: 0.5316\n",
      "Epoch 411/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0525 - mean_squared_error: 0.48 - 0s 103us/step - loss: 0.0765 - mean_squared_error: 0.5319\n",
      "Epoch 412/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1009 - mean_squared_error: 0.62 - 0s 97us/step - loss: 0.0765 - mean_squared_error: 0.5325\n",
      "Epoch 413/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1215 - mean_squared_error: 0.59 - 0s 101us/step - loss: 0.0764 - mean_squared_error: 0.5330\n",
      "Epoch 414/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0784 - mean_squared_error: 0.46 - 0s 95us/step - loss: 0.0764 - mean_squared_error: 0.5334\n",
      "Epoch 415/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1239 - mean_squared_error: 0.66 - 0s 97us/step - loss: 0.0764 - mean_squared_error: 0.5344\n",
      "Epoch 416/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.47 - 0s 98us/step - loss: 0.0762 - mean_squared_error: 0.5345\n",
      "Epoch 417/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0746 - mean_squared_error: 0.54 - 0s 110us/step - loss: 0.0761 - mean_squared_error: 0.5347\n",
      "Epoch 418/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0723 - mean_squared_error: 0.41 - 0s 96us/step - loss: 0.0760 - mean_squared_error: 0.5346\n",
      "Epoch 419/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1178 - mean_squared_error: 0.83 - 0s 97us/step - loss: 0.0759 - mean_squared_error: 0.5344\n",
      "Epoch 420/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0425 - mean_squared_error: 0.48 - 0s 99us/step - loss: 0.0758 - mean_squared_error: 0.5346\n",
      "Epoch 421/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0694 - mean_squared_error: 0.57 - 0s 97us/step - loss: 0.0756 - mean_squared_error: 0.5349\n",
      "Epoch 422/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1152 - mean_squared_error: 0.64 - 0s 96us/step - loss: 0.0756 - mean_squared_error: 0.5362\n",
      "Epoch 423/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1368 - mean_squared_error: 0.70 - 0s 99us/step - loss: 0.0754 - mean_squared_error: 0.5370\n",
      "Epoch 424/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0686 - mean_squared_error: 0.50 - 0s 104us/step - loss: 0.0752 - mean_squared_error: 0.5379\n",
      "Epoch 425/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.53 - 0s 99us/step - loss: 0.0751 - mean_squared_error: 0.5392\n",
      "Epoch 426/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0417 - mean_squared_error: 0.34 - 0s 98us/step - loss: 0.0750 - mean_squared_error: 0.5402\n",
      "Epoch 427/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0693 - mean_squared_error: 0.52 - 0s 98us/step - loss: 0.0749 - mean_squared_error: 0.5412\n",
      "Epoch 428/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0593 - mean_squared_error: 0.51 - 0s 100us/step - loss: 0.0749 - mean_squared_error: 0.5425\n",
      "Epoch 429/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.37 - 0s 101us/step - loss: 0.0747 - mean_squared_error: 0.5433\n",
      "Epoch 430/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0905 - mean_squared_error: 0.58 - 0s 93us/step - loss: 0.0747 - mean_squared_error: 0.5440\n",
      "Epoch 431/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0887 - mean_squared_error: 0.46 - 0s 96us/step - loss: 0.0746 - mean_squared_error: 0.5443\n",
      "Epoch 432/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0630 - mean_squared_error: 0.49 - 0s 99us/step - loss: 0.0746 - mean_squared_error: 0.5457\n",
      "Epoch 433/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0543 - mean_squared_error: 0.55 - 0s 99us/step - loss: 0.0745 - mean_squared_error: 0.5465\n",
      "Epoch 434/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0624 - mean_squared_error: 0.55 - 0s 99us/step - loss: 0.0744 - mean_squared_error: 0.5477\n",
      "Epoch 435/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.43 - 0s 100us/step - loss: 0.0743 - mean_squared_error: 0.5482\n",
      "Epoch 436/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0760 - mean_squared_error: 0.62 - 0s 98us/step - loss: 0.0743 - mean_squared_error: 0.5484\n",
      "Epoch 437/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0563 - mean_squared_error: 0.62 - 0s 92us/step - loss: 0.0742 - mean_squared_error: 0.5492\n",
      "Epoch 438/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0692 - mean_squared_error: 0.50 - 0s 106us/step - loss: 0.0742 - mean_squared_error: 0.5505\n",
      "Epoch 439/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.46 - 0s 127us/step - loss: 0.0741 - mean_squared_error: 0.5508\n",
      "Epoch 440/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0940 - mean_squared_error: 0.70 - 0s 152us/step - loss: 0.0741 - mean_squared_error: 0.5514\n",
      "Epoch 441/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0776 - mean_squared_error: 0.45 - 0s 129us/step - loss: 0.0741 - mean_squared_error: 0.5524\n",
      "Epoch 442/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0760 - mean_squared_error: 0.53 - 0s 131us/step - loss: 0.0740 - mean_squared_error: 0.5532\n",
      "Epoch 443/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0816 - mean_squared_error: 0.63 - 0s 121us/step - loss: 0.0740 - mean_squared_error: 0.5537\n",
      "Epoch 444/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0546 - mean_squared_error: 0.53 - 0s 199us/step - loss: 0.0739 - mean_squared_error: 0.5542\n",
      "Epoch 445/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0705 - mean_squared_error: 0.56 - 0s 125us/step - loss: 0.0739 - mean_squared_error: 0.5551\n",
      "Epoch 446/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0626 - mean_squared_error: 0.52 - 0s 137us/step - loss: 0.0739 - mean_squared_error: 0.5556\n",
      "Epoch 447/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0793 - mean_squared_error: 0.59 - 0s 104us/step - loss: 0.0738 - mean_squared_error: 0.5556\n",
      "Epoch 448/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0592 - mean_squared_error: 0.41 - 0s 143us/step - loss: 0.0738 - mean_squared_error: 0.5560\n",
      "Epoch 449/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0694 - mean_squared_error: 0.56 - 0s 163us/step - loss: 0.0738 - mean_squared_error: 0.5565\n",
      "Epoch 450/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0705 - mean_squared_error: 0.49 - 0s 201us/step - loss: 0.0737 - mean_squared_error: 0.5571\n",
      "Epoch 451/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0918 - mean_squared_error: 0.63 - 0s 175us/step - loss: 0.0737 - mean_squared_error: 0.5577\n",
      "Epoch 452/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0641 - mean_squared_error: 0.46 - 0s 229us/step - loss: 0.0737 - mean_squared_error: 0.5579\n",
      "Epoch 453/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0525 - mean_squared_error: 0.58 - 0s 112us/step - loss: 0.0736 - mean_squared_error: 0.5582\n",
      "Epoch 454/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0904 - mean_squared_error: 0.48 - 0s 148us/step - loss: 0.0737 - mean_squared_error: 0.5596\n",
      "Epoch 455/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0706 - mean_squared_error: 0.63 - 0s 169us/step - loss: 0.0736 - mean_squared_error: 0.5601\n",
      "Epoch 456/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0562 - mean_squared_error: 0.35 - 0s 156us/step - loss: 0.0736 - mean_squared_error: 0.5602\n",
      "Epoch 457/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0550 - mean_squared_error: 0.42 - 0s 137us/step - loss: 0.0736 - mean_squared_error: 0.5599\n",
      "Epoch 458/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1060 - mean_squared_error: 0.54 - 0s 134us/step - loss: 0.0735 - mean_squared_error: 0.5605\n",
      "Epoch 459/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0515 - mean_squared_error: 0.53 - 0s 142us/step - loss: 0.0735 - mean_squared_error: 0.5612\n",
      "Epoch 460/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0647 - mean_squared_error: 0.53 - 0s 175us/step - loss: 0.0735 - mean_squared_error: 0.5618\n",
      "Epoch 461/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0789 - mean_squared_error: 0.57 - 0s 136us/step - loss: 0.0735 - mean_squared_error: 0.5618\n",
      "Epoch 462/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0756 - mean_squared_error: 0.64 - 0s 150us/step - loss: 0.0735 - mean_squared_error: 0.5622\n",
      "Epoch 463/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0820 - mean_squared_error: 0.64 - 0s 150us/step - loss: 0.0734 - mean_squared_error: 0.5624\n",
      "Epoch 464/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0681 - mean_squared_error: 0.65 - 0s 154us/step - loss: 0.0734 - mean_squared_error: 0.5625\n",
      "Epoch 465/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0988 - mean_squared_error: 0.78 - 0s 149us/step - loss: 0.0734 - mean_squared_error: 0.5626\n",
      "Epoch 466/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0533 - mean_squared_error: 0.37 - 0s 189us/step - loss: 0.0734 - mean_squared_error: 0.5638\n",
      "Epoch 467/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0603 - mean_squared_error: 0.38 - 0s 181us/step - loss: 0.0733 - mean_squared_error: 0.5642\n",
      "Epoch 468/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0726 - mean_squared_error: 0.59 - 0s 160us/step - loss: 0.0734 - mean_squared_error: 0.5644\n",
      "Epoch 469/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0861 - mean_squared_error: 0.74 - 0s 237us/step - loss: 0.0733 - mean_squared_error: 0.5643\n",
      "Epoch 470/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0494 - mean_squared_error: 0.40 - 0s 161us/step - loss: 0.0733 - mean_squared_error: 0.5642\n",
      "Epoch 471/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0682 - mean_squared_error: 0.54 - 0s 158us/step - loss: 0.0733 - mean_squared_error: 0.5648\n",
      "Epoch 472/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0824 - mean_squared_error: 0.66 - 0s 135us/step - loss: 0.0733 - mean_squared_error: 0.5654\n",
      "Epoch 473/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0620 - mean_squared_error: 0.50 - 0s 115us/step - loss: 0.0733 - mean_squared_error: 0.5662\n",
      "Epoch 474/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0831 - mean_squared_error: 0.84 - 0s 160us/step - loss: 0.0733 - mean_squared_error: 0.5664\n",
      "Epoch 475/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0639 - mean_squared_error: 0.59 - 0s 201us/step - loss: 0.0732 - mean_squared_error: 0.5663\n",
      "Epoch 476/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0733 - mean_squared_error: 0.57 - 0s 121us/step - loss: 0.0732 - mean_squared_error: 0.5668\n",
      "Epoch 477/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0914 - mean_squared_error: 0.63 - 0s 135us/step - loss: 0.0732 - mean_squared_error: 0.5666\n",
      "Epoch 478/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0832 - mean_squared_error: 0.53 - 0s 124us/step - loss: 0.0732 - mean_squared_error: 0.5663\n",
      "Epoch 479/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0752 - mean_squared_error: 0.66 - 0s 147us/step - loss: 0.0732 - mean_squared_error: 0.5664\n",
      "Epoch 480/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0550 - mean_squared_error: 0.61 - 0s 183us/step - loss: 0.0732 - mean_squared_error: 0.5666\n",
      "Epoch 481/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0971 - mean_squared_error: 0.67 - 0s 145us/step - loss: 0.0732 - mean_squared_error: 0.5673\n",
      "Epoch 482/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0891 - mean_squared_error: 0.56 - 0s 127us/step - loss: 0.0732 - mean_squared_error: 0.5676\n",
      "Epoch 483/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0922 - mean_squared_error: 0.45 - 0s 131us/step - loss: 0.0732 - mean_squared_error: 0.5678\n",
      "Epoch 484/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0671 - mean_squared_error: 0.49 - 0s 152us/step - loss: 0.0732 - mean_squared_error: 0.5683\n",
      "Epoch 485/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.44 - 0s 185us/step - loss: 0.0731 - mean_squared_error: 0.5679\n",
      "Epoch 486/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.1105 - mean_squared_error: 0.57 - 0s 172us/step - loss: 0.0732 - mean_squared_error: 0.5679\n",
      "Epoch 487/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0754 - mean_squared_error: 0.60 - 0s 184us/step - loss: 0.0731 - mean_squared_error: 0.5679\n",
      "Epoch 488/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0863 - mean_squared_error: 0.67 - 0s 150us/step - loss: 0.0731 - mean_squared_error: 0.5679\n",
      "Epoch 489/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0773 - mean_squared_error: 0.63 - 0s 115us/step - loss: 0.0731 - mean_squared_error: 0.5674\n",
      "Epoch 490/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0586 - mean_squared_error: 0.62 - 0s 154us/step - loss: 0.0731 - mean_squared_error: 0.5675\n",
      "Epoch 491/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - ETA: 0s - loss: 0.1137 - mean_squared_error: 0.56 - 0s 100us/step - loss: 0.0731 - mean_squared_error: 0.5678\n",
      "Epoch 492/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0528 - mean_squared_error: 0.43 - 0s 195us/step - loss: 0.0731 - mean_squared_error: 0.5676\n",
      "Epoch 493/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0616 - mean_squared_error: 0.59 - 0s 127us/step - loss: 0.0731 - mean_squared_error: 0.5677\n",
      "Epoch 494/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0516 - mean_squared_error: 0.44 - 0s 105us/step - loss: 0.0730 - mean_squared_error: 0.5675\n",
      "Epoch 495/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0653 - mean_squared_error: 0.60 - 0s 106us/step - loss: 0.0730 - mean_squared_error: 0.5674\n",
      "Epoch 496/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0770 - mean_squared_error: 0.47 - 0s 108us/step - loss: 0.0730 - mean_squared_error: 0.5677\n",
      "Epoch 497/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.55 - 0s 108us/step - loss: 0.0730 - mean_squared_error: 0.5679\n",
      "Epoch 498/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0741 - mean_squared_error: 0.53 - 0s 107us/step - loss: 0.0730 - mean_squared_error: 0.5678\n",
      "Epoch 499/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0731 - mean_squared_error: 0.54 - 0s 169us/step - loss: 0.0730 - mean_squared_error: 0.5679\n",
      "Epoch 500/500\n",
      "176/176 [==============================] - ETA: 0s - loss: 0.0832 - mean_squared_error: 0.67 - 0s 125us/step - loss: 0.0730 - mean_squared_error: 0.5683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x185fe3e27b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = f1(param_name='glm_1')\n",
    "model.fit(x_train, y_train, epochs=500, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5481227216244245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score = mean_squared_error(y_test, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onnx.onnx_ml_pb2.ModelProto"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)\n",
    "# cross check import (f1p1 and f2p2 combination) - Is it possible to edit after the export-import flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx.save(model, './onnx_model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LogisticRegression': True}\n",
      "{'LinearRegression': True}\n"
     ]
    }
   ],
   "source": [
    "mapping = { \"KerasClassifier\": {\n",
    "    \"LogisticRegression\": True\n",
    "},\n",
    " \"KerasRegressor\": {\n",
    "     \"LinearRegression\": True\n",
    " }\n",
    "}\n",
    "\n",
    "name = \"LinearRegression\"\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    test = mapping[key]\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "def glm(**kwargs):  # Should param_name be optional or mandatory?\n",
    "\n",
    "    # kwargs.setdefault('param_name', 'glm_1')\n",
    "    params_json = json.load(open('../imly/architectures/sklearn/params.json')) # Remove and make it generic\n",
    "    params = params_json['params'][kwargs['param_name']]\n",
    "    kwargs.setdefault('params', params)\n",
    "    kwargs.setdefault('x_train', np.array([[1], [2]]))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(kwargs['params']['first_neuron'], # Change first_neuron to input_size\n",
    "                    input_dim=kwargs['x_train'].shape[1], # Find a better way to pass input_dim. Through params maybe?\n",
    "                    activation=kwargs['params']['activation']))\n",
    "\n",
    "    model.compile(optimizer=kwargs['params']['optimizer'],\n",
    "                  loss=kwargs['params']['losses'],\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17f81a0aeb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.__call__(param_name=\"log_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - ETA: 11 - 2s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5110251939386354, -4.287840857541651]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import random\n",
    "\n",
    "random.seed(7)\n",
    "# def create_model():\n",
    "def coeff_determination(y_true, y_pred): # Read and understand the workflow\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "params_json = json.load(open('../imly/architectures/sklearn/params.json')) # Remove and make it generic\n",
    "params = params_json['params']['log_reg']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1,  # Change first_neuron to input_size\n",
    "                input_dim=10,  # Find a better way to pass input_dim. Through params maybe?\n",
    "                activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[coeff_determination])  # Dealing with accuracy in regression models\n",
    "#     return model\n",
    "\n",
    "# model = KerasRegressor(build_fn=create_model)\n",
    "# model.fit(x_train, y_train)\n",
    "# model.score(x_test,y_test)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class kerasWrapper(build_fn):\n",
    "    def __init__():\n",
    "        self.build_fn = build_fn\n",
    "\n",
    "\n",
    "class myWrapper(kerasWrapper):\n",
    "    def __init__(self, build_fn, **kwargs):\n",
    "        super(kerasWrapper, self).__init__(build_fn=build_fn)\n",
    "        \n",
    "    def fit():\n",
    "        \n",
    "\n",
    "class create_model(**kwargs):\n",
    "    def __init__(self, kwargs):\n",
    "        try:\n",
    "            self.x_train = kwargs['x_train']\n",
    "        except KeyError:\n",
    "            self.x_train = None\n",
    "            \n",
    "    def __call__():\n",
    "        print(self.x_train)\n",
    "        \n",
    "\n",
    "\n",
    "build_fn = create_model()\n",
    "\n",
    "model = myWrapper(build_fn=build_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed --  __root__\n",
      "scope --  ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'declare_local_operator', 'declare_local_variable', 'delete_local_operator', 'delete_local_variable', 'find_sink_variables', 'get_local_variable_or_declare_one', 'get_onnx_variable_name', 'get_unique_operator_name', 'get_unique_variable_name', 'name', 'onnx_operator_names', 'onnx_variable_names', 'operators', 'parent_scopes', 'target_opset', 'variable_name_mapping', 'variables']\n",
      "Seed --  input\n",
      "Seed --  SklearnLinearClassifier\n",
      "Seed --  label\n",
      "Seed --  probabilities\n",
      "Seed --  LinearClassifier\n",
      "Seed --  probability_tensor\n",
      "Seed --  probability_tensor_normalized\n",
      "Seed --  Normalizer\n",
      "Seed --  ZipMap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 1.\n"
     ]
    }
   ],
   "source": [
    "from winmltools import convert_sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from onnxmltools.convert.common.data_types import FloatTensorType, Int64TensorType\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "onnx_model = convert_sklearn(model, 7, initial_types=[('input', FloatTensorType([1, 2]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "LinearRegression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SklearnKerasClassifier'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"LogisticRegression\"\n",
    "\n",
    "wrapper_mapping_json = json.load(open('../imly/wrappers/keras_wrapper_mapping.json'))\n",
    "\n",
    "for key, value in wrapper_mapping_json.items():\n",
    "    for name in value:\n",
    "        print(name)\n",
    "        if model_name == name:\n",
    "            wrapper = key\n",
    "            \n",
    "wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrappers.sklearn.keras_classifier.SklearnKerasClassifier"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "wrapper_class = 'SklearnKerasClassifier'\n",
    "\n",
    "path = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', wrapper_class)\n",
    "module_path = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', path).lower()\n",
    "package_name = module_path.split('_')[0]\n",
    "wrapper_name = '_'.join(module_path.split('_')[1:3])\n",
    "\n",
    "module_path = 'wrappers.' + package_name + '.' + wrapper_name\n",
    "module_path\n",
    "wrapper_module = __import__(module_path, fromlist=[wrapper_class])\n",
    "function = getattr(wrapper_module, wrapper_class)\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = 'sklearn_keras_classifier'\n",
    "'_'.join(module_path.split('_')[1:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198</td>\n",
       "      <td>4341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford galaxie 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>454.0</td>\n",
       "      <td>220</td>\n",
       "      <td>4354</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet impala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215</td>\n",
       "      <td>4312</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth fury iii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225</td>\n",
       "      <td>4425</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>pontiac catalina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>390.0</td>\n",
       "      <td>190</td>\n",
       "      <td>3850</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc ambassador dpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>383.0</td>\n",
       "      <td>170</td>\n",
       "      <td>3563</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>dodge challenger se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>340.0</td>\n",
       "      <td>160</td>\n",
       "      <td>3609</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth 'cuda 340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3761</td>\n",
       "      <td>9.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet monte carlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225</td>\n",
       "      <td>3086</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick estate wagon (sw)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2372</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>toyota corona mark ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>198.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2833</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth duster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2774</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc hornet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>85</td>\n",
       "      <td>2587</td>\n",
       "      <td>16.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford maverick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2130</td>\n",
       "      <td>14.5</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>datsun pl510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>46</td>\n",
       "      <td>1835</td>\n",
       "      <td>20.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>volkswagen 1131 deluxe sedan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>87</td>\n",
       "      <td>2672</td>\n",
       "      <td>17.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>peugeot 504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2430</td>\n",
       "      <td>14.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>audi 100 ls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2375</td>\n",
       "      <td>17.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>saab 99e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>113</td>\n",
       "      <td>2234</td>\n",
       "      <td>12.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>bmw 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2648</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc gremlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>215</td>\n",
       "      <td>4615</td>\n",
       "      <td>14.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford f250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>200</td>\n",
       "      <td>4376</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevy c20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>210</td>\n",
       "      <td>4382</td>\n",
       "      <td>13.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>dodge d200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>hi 1200d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2130</td>\n",
       "      <td>14.5</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>datsun pl510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2640</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet cavalier wagon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>34.0</td>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2395</td>\n",
       "      <td>18.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet cavalier 2-door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>85</td>\n",
       "      <td>2575</td>\n",
       "      <td>16.2</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>pontiac j2000 se hatchback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>29.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2525</td>\n",
       "      <td>16.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>dodge aries se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2735</td>\n",
       "      <td>18.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>pontiac phoenix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>92</td>\n",
       "      <td>2865</td>\n",
       "      <td>16.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>ford fairmont futura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>23.0</td>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>?</td>\n",
       "      <td>3035</td>\n",
       "      <td>20.5</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>amc concord dl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>74</td>\n",
       "      <td>1980</td>\n",
       "      <td>15.3</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>volkswagen rabbit l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>37.0</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>68</td>\n",
       "      <td>2025</td>\n",
       "      <td>18.2</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>mazda glc custom l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1970</td>\n",
       "      <td>17.6</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>mazda glc custom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>63</td>\n",
       "      <td>2125</td>\n",
       "      <td>14.7</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth horizon miser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>70</td>\n",
       "      <td>2125</td>\n",
       "      <td>17.3</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>mercury lynx l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2160</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>nissan stanza xe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>75</td>\n",
       "      <td>2205</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>honda accord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>34.0</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>70</td>\n",
       "      <td>2245</td>\n",
       "      <td>16.9</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>toyota corolla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67</td>\n",
       "      <td>1965</td>\n",
       "      <td>15.0</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>honda civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67</td>\n",
       "      <td>1965</td>\n",
       "      <td>15.7</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>honda civic (auto)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67</td>\n",
       "      <td>1995</td>\n",
       "      <td>16.2</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>datsun 310 gx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>181.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2945</td>\n",
       "      <td>16.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>buick century limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>38.0</td>\n",
       "      <td>6</td>\n",
       "      <td>262.0</td>\n",
       "      <td>85</td>\n",
       "      <td>3015</td>\n",
       "      <td>17.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>oldsmobile cutlass ciera (diesel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>156.0</td>\n",
       "      <td>92</td>\n",
       "      <td>2585</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>chrysler lebaron medallion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>232.0</td>\n",
       "      <td>112</td>\n",
       "      <td>2835</td>\n",
       "      <td>14.7</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>ford granada l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>144.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2665</td>\n",
       "      <td>13.9</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>toyota celica gt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2370</td>\n",
       "      <td>13.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>dodge charger 2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2950</td>\n",
       "      <td>17.3</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet camaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>ford mustang gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52</td>\n",
       "      <td>2130</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>vw pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>dodge rampage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>ford ranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>chevy s-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement horsepower  weight  acceleration  \\\n",
       "0    18.0          8         307.0        130    3504          12.0   \n",
       "1    15.0          8         350.0        165    3693          11.5   \n",
       "2    18.0          8         318.0        150    3436          11.0   \n",
       "3    16.0          8         304.0        150    3433          12.0   \n",
       "4    17.0          8         302.0        140    3449          10.5   \n",
       "5    15.0          8         429.0        198    4341          10.0   \n",
       "6    14.0          8         454.0        220    4354           9.0   \n",
       "7    14.0          8         440.0        215    4312           8.5   \n",
       "8    14.0          8         455.0        225    4425          10.0   \n",
       "9    15.0          8         390.0        190    3850           8.5   \n",
       "10   15.0          8         383.0        170    3563          10.0   \n",
       "11   14.0          8         340.0        160    3609           8.0   \n",
       "12   15.0          8         400.0        150    3761           9.5   \n",
       "13   14.0          8         455.0        225    3086          10.0   \n",
       "14   24.0          4         113.0         95    2372          15.0   \n",
       "15   22.0          6         198.0         95    2833          15.5   \n",
       "16   18.0          6         199.0         97    2774          15.5   \n",
       "17   21.0          6         200.0         85    2587          16.0   \n",
       "18   27.0          4          97.0         88    2130          14.5   \n",
       "19   26.0          4          97.0         46    1835          20.5   \n",
       "20   25.0          4         110.0         87    2672          17.5   \n",
       "21   24.0          4         107.0         90    2430          14.5   \n",
       "22   25.0          4         104.0         95    2375          17.5   \n",
       "23   26.0          4         121.0        113    2234          12.5   \n",
       "24   21.0          6         199.0         90    2648          15.0   \n",
       "25   10.0          8         360.0        215    4615          14.0   \n",
       "26   10.0          8         307.0        200    4376          15.0   \n",
       "27   11.0          8         318.0        210    4382          13.5   \n",
       "28    9.0          8         304.0        193    4732          18.5   \n",
       "29   27.0          4          97.0         88    2130          14.5   \n",
       "..    ...        ...           ...        ...     ...           ...   \n",
       "368  27.0          4         112.0         88    2640          18.6   \n",
       "369  34.0          4         112.0         88    2395          18.0   \n",
       "370  31.0          4         112.0         85    2575          16.2   \n",
       "371  29.0          4         135.0         84    2525          16.0   \n",
       "372  27.0          4         151.0         90    2735          18.0   \n",
       "373  24.0          4         140.0         92    2865          16.4   \n",
       "374  23.0          4         151.0          ?    3035          20.5   \n",
       "375  36.0          4         105.0         74    1980          15.3   \n",
       "376  37.0          4          91.0         68    2025          18.2   \n",
       "377  31.0          4          91.0         68    1970          17.6   \n",
       "378  38.0          4         105.0         63    2125          14.7   \n",
       "379  36.0          4          98.0         70    2125          17.3   \n",
       "380  36.0          4         120.0         88    2160          14.5   \n",
       "381  36.0          4         107.0         75    2205          14.5   \n",
       "382  34.0          4         108.0         70    2245          16.9   \n",
       "383  38.0          4          91.0         67    1965          15.0   \n",
       "384  32.0          4          91.0         67    1965          15.7   \n",
       "385  38.0          4          91.0         67    1995          16.2   \n",
       "386  25.0          6         181.0        110    2945          16.4   \n",
       "387  38.0          6         262.0         85    3015          17.0   \n",
       "388  26.0          4         156.0         92    2585          14.5   \n",
       "389  22.0          6         232.0        112    2835          14.7   \n",
       "390  32.0          4         144.0         96    2665          13.9   \n",
       "391  36.0          4         135.0         84    2370          13.0   \n",
       "392  27.0          4         151.0         90    2950          17.3   \n",
       "393  27.0          4         140.0         86    2790          15.6   \n",
       "394  44.0          4          97.0         52    2130          24.6   \n",
       "395  32.0          4         135.0         84    2295          11.6   \n",
       "396  28.0          4         120.0         79    2625          18.6   \n",
       "397  31.0          4         119.0         82    2720          19.4   \n",
       "\n",
       "     model year  origin                           car name  \n",
       "0            70       1          chevrolet chevelle malibu  \n",
       "1            70       1                  buick skylark 320  \n",
       "2            70       1                 plymouth satellite  \n",
       "3            70       1                      amc rebel sst  \n",
       "4            70       1                        ford torino  \n",
       "5            70       1                   ford galaxie 500  \n",
       "6            70       1                   chevrolet impala  \n",
       "7            70       1                  plymouth fury iii  \n",
       "8            70       1                   pontiac catalina  \n",
       "9            70       1                 amc ambassador dpl  \n",
       "10           70       1                dodge challenger se  \n",
       "11           70       1                 plymouth 'cuda 340  \n",
       "12           70       1              chevrolet monte carlo  \n",
       "13           70       1            buick estate wagon (sw)  \n",
       "14           70       3              toyota corona mark ii  \n",
       "15           70       1                    plymouth duster  \n",
       "16           70       1                         amc hornet  \n",
       "17           70       1                      ford maverick  \n",
       "18           70       3                       datsun pl510  \n",
       "19           70       2       volkswagen 1131 deluxe sedan  \n",
       "20           70       2                        peugeot 504  \n",
       "21           70       2                        audi 100 ls  \n",
       "22           70       2                           saab 99e  \n",
       "23           70       2                           bmw 2002  \n",
       "24           70       1                        amc gremlin  \n",
       "25           70       1                          ford f250  \n",
       "26           70       1                          chevy c20  \n",
       "27           70       1                         dodge d200  \n",
       "28           70       1                           hi 1200d  \n",
       "29           71       3                       datsun pl510  \n",
       "..          ...     ...                                ...  \n",
       "368          82       1           chevrolet cavalier wagon  \n",
       "369          82       1          chevrolet cavalier 2-door  \n",
       "370          82       1         pontiac j2000 se hatchback  \n",
       "371          82       1                     dodge aries se  \n",
       "372          82       1                    pontiac phoenix  \n",
       "373          82       1               ford fairmont futura  \n",
       "374          82       1                     amc concord dl  \n",
       "375          82       2                volkswagen rabbit l  \n",
       "376          82       3                 mazda glc custom l  \n",
       "377          82       3                   mazda glc custom  \n",
       "378          82       1             plymouth horizon miser  \n",
       "379          82       1                     mercury lynx l  \n",
       "380          82       3                   nissan stanza xe  \n",
       "381          82       3                       honda accord  \n",
       "382          82       3                     toyota corolla  \n",
       "383          82       3                        honda civic  \n",
       "384          82       3                 honda civic (auto)  \n",
       "385          82       3                      datsun 310 gx  \n",
       "386          82       1              buick century limited  \n",
       "387          82       1  oldsmobile cutlass ciera (diesel)  \n",
       "388          82       1         chrysler lebaron medallion  \n",
       "389          82       1                     ford granada l  \n",
       "390          82       3                   toyota celica gt  \n",
       "391          82       1                  dodge charger 2.2  \n",
       "392          82       1                   chevrolet camaro  \n",
       "393          82       1                    ford mustang gl  \n",
       "394          82       2                          vw pickup  \n",
       "395          82       1                      dodge rampage  \n",
       "396          82       1                        ford ranger  \n",
       "397          82       1                         chevy s-10  \n",
       "\n",
       "[398 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"../data/uci_auto_mpg.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=0, index_col=0)\n",
    "data\n",
    "# frames = [X, Y]\n",
    "# data = pd.concat(frames, axis=1)\n",
    "# data.to_csv('../data/uci_auto_mpg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
