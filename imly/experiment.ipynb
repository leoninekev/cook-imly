{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments using IMLY ###\n",
    "\n",
    "This notebook contains experimental runs of IMLY with different datasets.  \n",
    "The readings of these experiments can be referred to in this [sheet](https://docs.google.com/spreadsheets/d/1E5jcq2w42gN8bMIaeaRJpAdhgSVN-2XDJ_YTHe4qfwY/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #1\n",
    "\n",
    "#### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From talos.py ---  {'lr': [2, 10, 30], 'first_neuron': [1], 'batch_size': [10], 'epochs': [10], 'weight_regulizer': [None], 'emb_output_dims': [None], 'optimizer': ['nadam'], 'losses': ['mae', 'mse'], 'activation': ['linear'], 'model_name': ['SklearnKerasRegressor']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - ETA: 0s - loss: 157.1494 - mean_absolute_error: 157.149 - 0s 621us/step - loss: 151.9254 - mean_absolute_error: 151.9254\n",
      "266/266 [==============================] - ETA:  - 0s 117us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #2\n",
    "\n",
    "#### UCI Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "529/529 [==============================] - ETA: 2s - loss: 0.6968 - binary_crossentropy: 0.696 - 0s 331us/step - loss: 0.6899 - binary_crossentropy: 0.6899\n",
      "794/794 [==============================] - ETA:  - 0s 71us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"uci_abalone\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\" if path.exists(\"../data/abalone.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #3\n",
    "\n",
    "#### UCI Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.9766 - binary_crossentropy: 1.976 - 1s 19ms/step - loss: 1.9969 - binary_crossentropy: 1.9969\n",
      "60/60 [==============================] - ETA:  - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #4\n",
    "\n",
    "#### UCI Adult salary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:33<00:00, 33.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "18088/18088 [==============================] - ETA: 10:30 - loss: 4.5332 - binary_crossentropy: 4.53 - ETA: 37s - loss: 3.7332 - binary_crossentropy: 3.7332 - ETA: 22s - loss: 3.8385 - binary_crossentropy: 3.83 - ETA: 14s - loss: 3.8105 - binary_crossentropy: 3.81 - ETA: 10s - loss: 3.8137 - binary_crossentropy: 3.81 - ETA: 7s - loss: 3.8221 - binary_crossentropy: 3.8221 - ETA: 6s - loss: 3.8760 - binary_crossentropy: 3.876 - ETA: 5s - loss: 3.8771 - binary_crossentropy: 3.877 - ETA: 4s - loss: 3.8629 - binary_crossentropy: 3.862 - ETA: 4s - loss: 3.8735 - binary_crossentropy: 3.873 - ETA: 3s - loss: 3.8697 - binary_crossentropy: 3.869 - ETA: 3s - loss: 3.8934 - binary_crossentropy: 3.893 - ETA: 3s - loss: 3.8981 - binary_crossentropy: 3.898 - ETA: 2s - loss: 3.9148 - binary_crossentropy: 3.914 - ETA: 2s - loss: 3.9225 - binary_crossentropy: 3.922 - ETA: 2s - loss: 3.9327 - binary_crossentropy: 3.932 - ETA: 2s - loss: 3.9087 - binary_crossentropy: 3.908 - ETA: 1s - loss: 3.9333 - binary_crossentropy: 3.933 - ETA: 1s - loss: 3.9370 - binary_crossentropy: 3.937 - ETA: 1s - loss: 3.9250 - binary_crossentropy: 3.925 - ETA: 1s - loss: 3.9088 - binary_crossentropy: 3.908 - ETA: 1s - loss: 3.9022 - binary_crossentropy: 3.902 - ETA: 1s - loss: 3.8959 - binary_crossentropy: 3.895 - ETA: 1s - loss: 3.9257 - binary_crossentropy: 3.925 - ETA: 0s - loss: 3.9213 - binary_crossentropy: 3.921 - ETA: 0s - loss: 3.9276 - binary_crossentropy: 3.927 - ETA: 0s - loss: 3.9346 - binary_crossentropy: 3.934 - ETA: 0s - loss: 3.9486 - binary_crossentropy: 3.948 - ETA: 0s - loss: 3.9456 - binary_crossentropy: 3.945 - ETA: 0s - loss: 3.9489 - binary_crossentropy: 3.948 - ETA: 0s - loss: 3.9473 - binary_crossentropy: 3.947 - ETA: 0s - loss: 3.9499 - binary_crossentropy: 3.949 - ETA: 0s - loss: 3.9469 - binary_crossentropy: 3.946 - ETA: 0s - loss: 3.9507 - binary_crossentropy: 3.950 - ETA: 0s - loss: 3.9612 - binary_crossentropy: 3.961 - 3s 160us/step - loss: 3.9609 - binary_crossentropy: 3.9609\n",
      "27134/27134 [==============================] - ETA: 27 - ETA: 1 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 38us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_adult_salary\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\" \", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)\n",
    "\n",
    "# Split the dataset into test and train datasets\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #5\n",
    "\n",
    "#### UCI Ad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "943/943 [==============================] - ETA: 6s - loss: 0.0594 - binary_crossentropy: 0.059 - ETA: 0s - loss: 0.1452 - binary_crossentropy: 0.145 - 0s 345us/step - loss: 0.1602 - binary_crossentropy: 0.1602\n",
      "1416/1416 [==============================] - ETA:  - ETA:  - 0s 61us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #6\n",
    "\n",
    "#### UCI Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LinearRegression'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "model_name = 'linear_regression'\n",
    "model_mappings = {\n",
    "    'linear_regression': 'LinearRegression',\n",
    "    'logistic_regression': 'LogisticRegression'\n",
    "}\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "for key, value in model_mappings.items():\n",
    "    if key == model_name:\n",
    "        name = value\n",
    "\n",
    "module = __import__('sklearn.linear_model', fromlist=[name])\n",
    "imported_module = getattr(module, name)\n",
    "model = imported_module\n",
    "\n",
    "primal_model = model()\n",
    "\n",
    "# Primal\n",
    "primal_model.fit(x_train, y_train)\n",
    "primal_model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data\n",
    "# sc = StandardScaler()\n",
    "# diabetes.data = sc.fit_transform(diabetes.data)\n",
    "\n",
    "X = diabetes.data\n",
    "Y = diabetes.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "# # Split the data into training/testing sets\n",
    "# x_train = diabetes_X[:-20]\n",
    "# x_test = diabetes_X[-20:]\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# y_train = diabetes.target[:-20]\n",
    "# y_test = diabetes.target[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = np.column_stack([X,Y])\n",
    "# np.savetxt(\"diabetes.csv\", temp_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shakk\\\\Anaconda2\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\winmltools\\\\__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import winmltools\n",
    "winmltools.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "\n",
    "def f1(**kwargs):\n",
    "    params_json = json.load(open('../imly/architectures/sklearn/params.json'))\n",
    "    params = params_json['params']\n",
    "    kwargs.setdefault('params', params)\n",
    "    kwargs.setdefault('x_train', np.array([[1], [2]]))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(kwargs['params']['first_neuron'],\n",
    "                    input_dim=kwargs['x_train'].shape[1],\n",
    "                    activation=kwargs['params']['activation']))\n",
    "\n",
    "    model.compile(optimizer=kwargs['params']['optimizer'],\n",
    "                  loss=kwargs['params']['losses'],\n",
    "                  metrics=['acc'])\n",
    "    onnx_model = onnxmltools.convert_keras(model, target_opset=8)\n",
    "    print(type(model))\n",
    "    onnx_model\n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.sequential.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onnx.onnx_ml_pb2.ModelProto"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)\n",
    "# cross check import (f1p1 and f2p2 combination) - Is it possible to edit after the export-import flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx.save(model, './onnx_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
