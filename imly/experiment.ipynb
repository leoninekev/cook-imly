{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments using IMLY ###\n",
    "\n",
    "This notebook contains experimental runs of IMLY with different datasets.  \n",
    "The readings of these experiments can be referred to in this [sheet](https://docs.google.com/spreadsheets/d/1E5jcq2w42gN8bMIaeaRJpAdhgSVN-2XDJ_YTHe4qfwY/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #1\n",
    "\n",
    "#### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - ETA: 0s - loss: 157.1494 - mean_absolute_error: 157.149 - 0s 710us/step - loss: 151.9254 - mean_absolute_error: 151.9254\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SklearnKerasRegressor' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-59d4d8242353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mexperiment_automation_script\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdopify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'linear_regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\experiment_automation_script.py\u001b[0m in \u001b[0;36mdopify\u001b[1;34m(dataset_info, model_name, X, Y, test_size)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m     \u001b[0mkeras_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m     \u001b[1;31m# keras_score = round(keras_score * 100, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SklearnKerasRegressor' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #2\n",
    "\n",
    "#### UCI Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "529/529 [==============================] - ETA: 1s - loss: 0.6968 - binary_crossentropy: 0.696 - 0s 295us/step - loss: 0.6899 - binary_crossentropy: 0.6899\n",
      "794/794 [==============================] - ETA:  - 0s 59us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"uci_abalone\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\" if path.exists(\"../data/abalone.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #3\n",
    "\n",
    "#### UCI Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b28462c06e75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mexperiment_automation_script\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdopify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic_regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\experiment_automation_script.py\u001b[0m in \u001b[0;36mdopify\u001b[1;34m(dataset_info, model_name, X, Y, test_size)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[1;31m# Talos accepts only numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m     \u001b[0mkeras_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;31m# keras_score = round(keras_score * 100, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m                                         performance_metric=self.performance_metric) \n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# self.model.fit(x_train, y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Why? - 'classes_' missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m               not isinstance(self.build_fn, types.MethodType)):\n\u001b[0;32m    138\u001b[0m             self.model = self.build_fn(\n\u001b[1;32m--> 139\u001b[1;33m                 **self.filter_sk_params(self.build_fn.__call__))\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "40/40 [==============================] - ETA: 1s - loss: 0.9877 - acc: 0.700 - 0s 12ms/step - loss: 1.3947 - acc: 0.5500\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.7549 - acc: 0.400 - 0s 391us/step - loss: 1.3838 - acc: 0.5500\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2139 - acc: 0.600 - 0s 391us/step - loss: 1.3706 - acc: 0.5500\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2532 - acc: 0.600 - 0s 390us/step - loss: 1.3582 - acc: 0.5500\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.8960 - acc: 0.300 - 0s 781us/step - loss: 1.3481 - acc: 0.5500\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6283 - acc: 0.400 - 0s 782us/step - loss: 1.3347 - acc: 0.5500\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.5119 - acc: 0.500 - 0s 390us/step - loss: 1.3230 - acc: 0.5500\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.5232 - acc: 0.500 - 0s 781us/step - loss: 1.3116 - acc: 0.5500\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9319 - acc: 0.700 - 0s 781us/step - loss: 1.2994 - acc: 0.5500\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1990 - acc: 0.600 - 0s 781us/step - loss: 1.2893 - acc: 0.5500\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.5619 - acc: 0.400 - 0s 391us/step - loss: 1.2813 - acc: 0.5500\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1983 - acc: 0.600 - 0s 781us/step - loss: 1.2688 - acc: 0.5500\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7251 - acc: 0.800 - 0s 781us/step - loss: 1.2582 - acc: 0.5500\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.3800 - acc: 0.500 - 0s 781us/step - loss: 1.2500 - acc: 0.5500\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6680 - acc: 0.800 - 0s 782us/step - loss: 1.2383 - acc: 0.5500\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.5570 - acc: 0.400 - 0s 781us/step - loss: 1.2320 - acc: 0.5500\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.3083 - acc: 0.500 - 0s 391us/step - loss: 1.2222 - acc: 0.5500\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.5044 - acc: 0.400 - 0s 390us/step - loss: 1.2147 - acc: 0.5500\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0974 - acc: 0.600 - 0s 391us/step - loss: 1.2048 - acc: 0.5500\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1481 - acc: 0.600 - 0s 391us/step - loss: 1.1971 - acc: 0.5500\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.4488 - acc: 0.400 - 0s 781us/step - loss: 1.1883 - acc: 0.5500\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.7082 - acc: 0.300 - 0s 781us/step - loss: 1.1813 - acc: 0.5500\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.4371 - acc: 0.400 - 0s 390us/step - loss: 1.1729 - acc: 0.5500\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2407 - acc: 0.500 - 0s 391us/step - loss: 1.1645 - acc: 0.5500\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1006 - acc: 0.600 - 0s 781us/step - loss: 1.1579 - acc: 0.5500\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0377 - acc: 0.600 - 0s 1ms/step - loss: 1.1503 - acc: 0.5500\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.4420 - acc: 0.400 - 0s 390us/step - loss: 1.1431 - acc: 0.5500\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.5752 - acc: 0.300 - 0s 781us/step - loss: 1.1365 - acc: 0.5500\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2481 - acc: 0.500 - 0s 390us/step - loss: 1.1283 - acc: 0.5500\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.3275 - acc: 0.400 - 0s 781us/step - loss: 1.1215 - acc: 0.5500\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.3253 - acc: 0.400 - 0s 390us/step - loss: 1.1152 - acc: 0.5500\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6353 - acc: 0.200 - 0s 391us/step - loss: 1.1087 - acc: 0.5500\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5547 - acc: 0.900 - 0s 781us/step - loss: 1.1007 - acc: 0.5500\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0366 - acc: 0.600 - 0s 0us/step - loss: 1.0949 - acc: 0.5500\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6267 - acc: 0.200 - 0s 781us/step - loss: 1.0901 - acc: 0.5500\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0055 - acc: 0.600 - 0s 781us/step - loss: 1.0815 - acc: 0.5500\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9657 - acc: 0.600 - 0s 390us/step - loss: 1.0761 - acc: 0.5500\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8405 - acc: 0.700 - 0s 390us/step - loss: 1.0688 - acc: 0.5500\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1409 - acc: 0.500 - 0s 781us/step - loss: 1.0636 - acc: 0.5500\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.5412 - acc: 0.200 - 0s 390us/step - loss: 1.0580 - acc: 0.5500\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2533 - acc: 0.400 - 0s 391us/step - loss: 1.0514 - acc: 0.5500\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2276 - acc: 0.400 - 0s 390us/step - loss: 1.0455 - acc: 0.5500\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1429 - acc: 0.500 - 0s 781us/step - loss: 1.0396 - acc: 0.5500\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2542 - acc: 0.400 - 0s 391us/step - loss: 1.0337 - acc: 0.5500\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9523 - acc: 0.600 - 0s 391us/step - loss: 1.0277 - acc: 0.5500\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2267 - acc: 0.400 - 0s 781us/step - loss: 1.0224 - acc: 0.5500\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0794 - acc: 0.500 - 0s 391us/step - loss: 1.0168 - acc: 0.5500\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0774 - acc: 0.500 - 0s 391us/step - loss: 1.0110 - acc: 0.5500\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9467 - acc: 0.600 - 0s 391us/step - loss: 1.0049 - acc: 0.5500\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6917 - acc: 0.800 - 0s 391us/step - loss: 0.9994 - acc: 0.5500\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7043 - acc: 0.800 - 0s 390us/step - loss: 0.9940 - acc: 0.5500\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9668 - acc: 0.500 - 0s 391us/step - loss: 0.9887 - acc: 0.5500\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1330 - acc: 0.400 - 0s 390us/step - loss: 0.9835 - acc: 0.5500\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0365 - acc: 0.500 - 0s 391us/step - loss: 0.9782 - acc: 0.5500\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0351 - acc: 0.500 - 0s 391us/step - loss: 0.9723 - acc: 0.5500\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9162 - acc: 0.600 - 0s 781us/step - loss: 0.9668 - acc: 0.5500\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9050 - acc: 0.600 - 0s 391us/step - loss: 0.9618 - acc: 0.5500\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9058 - acc: 0.600 - 0s 391us/step - loss: 0.9562 - acc: 0.5500\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9474 - acc: 0.600 - 0s 391us/step - loss: 0.9512 - acc: 0.5500\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0255 - acc: 0.500 - 0s 391us/step - loss: 0.9463 - acc: 0.5500\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9298 - acc: 0.600 - 0s 391us/step - loss: 0.9405 - acc: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9020 - acc: 0.600 - 0s 391us/step - loss: 0.9359 - acc: 0.5500\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0838 - acc: 0.400 - 0s 390us/step - loss: 0.9303 - acc: 0.5500\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2043 - acc: 0.300 - 0s 391us/step - loss: 0.9260 - acc: 0.5500\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8818 - acc: 0.600 - 0s 391us/step - loss: 0.9203 - acc: 0.5500\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1933 - acc: 0.300 - 0s 391us/step - loss: 0.9156 - acc: 0.5500\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8611 - acc: 0.600 - 0s 390us/step - loss: 0.9101 - acc: 0.5500\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0028 - acc: 0.500 - 0s 390us/step - loss: 0.9055 - acc: 0.5500\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7266 - acc: 0.700 - 0s 390us/step - loss: 0.9003 - acc: 0.5500\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7422 - acc: 0.700 - 0s 391us/step - loss: 0.8957 - acc: 0.5500\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9226 - acc: 0.500 - 0s 391us/step - loss: 0.8911 - acc: 0.5500\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9873 - acc: 0.500 - 0s 391us/step - loss: 0.8862 - acc: 0.5500\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9540 - acc: 0.500 - 0s 781us/step - loss: 0.8815 - acc: 0.5500\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8920 - acc: 0.500 - 0s 391us/step - loss: 0.8766 - acc: 0.5500\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9113 - acc: 0.500 - 0s 391us/step - loss: 0.8716 - acc: 0.5500\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8084 - acc: 0.600 - 0s 391us/step - loss: 0.8673 - acc: 0.5500\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8064 - acc: 0.600 - 0s 0us/step - loss: 0.8624 - acc: 0.5500\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9803 - acc: 0.400 - 0s 390us/step - loss: 0.8579 - acc: 0.5500\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9880 - acc: 0.400 - 0s 391us/step - loss: 0.8533 - acc: 0.5500\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8907 - acc: 0.500 - 0s 391us/step - loss: 0.8488 - acc: 0.5500\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.0070 - acc: 0.400 - 0s 391us/step - loss: 0.8448 - acc: 0.5500\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8679 - acc: 0.500 - 0s 391us/step - loss: 0.8396 - acc: 0.5500\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9073 - acc: 0.500 - 0s 391us/step - loss: 0.8353 - acc: 0.5500\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8935 - acc: 0.500 - 0s 391us/step - loss: 0.8309 - acc: 0.5500\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9645 - acc: 0.400 - 0s 391us/step - loss: 0.8266 - acc: 0.5500\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7009 - acc: 0.700 - 0s 391us/step - loss: 0.8223 - acc: 0.5500\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9677 - acc: 0.400 - 0s 391us/step - loss: 0.8180 - acc: 0.5500\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8850 - acc: 0.500 - 0s 390us/step - loss: 0.8135 - acc: 0.5500\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6167 - acc: 0.800 - 0s 391us/step - loss: 0.8092 - acc: 0.5500\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7819 - acc: 0.600 - 0s 391us/step - loss: 0.8050 - acc: 0.5500\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6771 - acc: 0.700 - 0s 391us/step - loss: 0.8008 - acc: 0.5500\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9917 - acc: 0.300 - 0s 391us/step - loss: 0.7969 - acc: 0.5500\n",
      "Epoch 93/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9308 - acc: 0.400 - 0s 782us/step - loss: 0.7926 - acc: 0.5500\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8535 - acc: 0.500 - 0s 390us/step - loss: 0.7885 - acc: 0.5500\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6659 - acc: 0.700 - 0s 391us/step - loss: 0.7841 - acc: 0.5500\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7299 - acc: 0.600 - 0s 391us/step - loss: 0.7802 - acc: 0.5500\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7933 - acc: 0.500 - 0s 391us/step - loss: 0.7762 - acc: 0.5500\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8671 - acc: 0.400 - 0s 390us/step - loss: 0.7722 - acc: 0.5500\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7710 - acc: 0.500 - 0s 391us/step - loss: 0.7680 - acc: 0.5500\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7371 - acc: 0.600 - 0s 391us/step - loss: 0.7639 - acc: 0.5500\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5743 - acc: 0.800 - 0s 390us/step - loss: 0.7601 - acc: 0.5500\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6565 - acc: 0.700 - 0s 391us/step - loss: 0.7564 - acc: 0.5500\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7362 - acc: 0.600 - 0s 391us/step - loss: 0.7522 - acc: 0.5500\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5652 - acc: 0.800 - 0s 391us/step - loss: 0.7482 - acc: 0.5500\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5052 - acc: 0.900 - 0s 390us/step - loss: 0.7444 - acc: 0.5500\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7732 - acc: 0.500 - 0s 391us/step - loss: 0.7408 - acc: 0.5500\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7169 - acc: 0.600 - 0s 391us/step - loss: 0.7367 - acc: 0.5500\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6910 - acc: 0.600 - 0s 781us/step - loss: 0.7330 - acc: 0.5500\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6482 - acc: 0.700 - 0s 391us/step - loss: 0.7291 - acc: 0.5500\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6194 - acc: 0.700 - 0s 391us/step - loss: 0.7254 - acc: 0.5500\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4904 - acc: 0.900 - 0s 781us/step - loss: 0.7217 - acc: 0.5500\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7380 - acc: 0.500 - 0s 781us/step - loss: 0.7181 - acc: 0.5500\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7372 - acc: 0.500 - 0s 781us/step - loss: 0.7142 - acc: 0.5500\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6117 - acc: 0.700 - 0s 781us/step - loss: 0.7105 - acc: 0.5500\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7786 - acc: 0.400 - 0s 391us/step - loss: 0.7070 - acc: 0.5500\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8372 - acc: 0.300 - 0s 781us/step - loss: 0.7037 - acc: 0.5500\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7473 - acc: 0.500 - 0s 390us/step - loss: 0.6997 - acc: 0.5500\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5921 - acc: 0.700 - 0s 781us/step - loss: 0.6961 - acc: 0.5500\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6121 - acc: 0.700 - 0s 391us/step - loss: 0.6926 - acc: 0.5500\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7115 - acc: 0.500 - 0s 390us/step - loss: 0.6895 - acc: 0.5500\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7336 - acc: 0.500 - 0s 391us/step - loss: 0.6855 - acc: 0.5500\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 0.6378 - acc: 0.600 - 0s 391us/step - loss: 0.6820 - acc: 0.5500\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7650 - acc: 0.400 - 0s 391us/step - loss: 0.6787 - acc: 0.5500\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7781 - acc: 0.400 - 0s 391us/step - loss: 0.6752 - acc: 0.5500\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7509 - acc: 0.400 - 0s 391us/step - loss: 0.6718 - acc: 0.5500\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7017 - acc: 0.500 - 0s 391us/step - loss: 0.6683 - acc: 0.5500\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7666 - acc: 0.400 - 0s 391us/step - loss: 0.6650 - acc: 0.5500\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6777 - acc: 0.500 - 0s 391us/step - loss: 0.6617 - acc: 0.5500\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6202 - acc: 0.600 - 0s 0us/step - loss: 0.6582 - acc: 0.5500\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5760 - acc: 0.700 - 0s 391us/step - loss: 0.6549 - acc: 0.5500\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5863 - acc: 0.700 - 0s 391us/step - loss: 0.6516 - acc: 0.5500\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6275 - acc: 0.600 - 0s 391us/step - loss: 0.6484 - acc: 0.5500\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6263 - acc: 0.600 - 0s 391us/step - loss: 0.6453 - acc: 0.5500\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6194 - acc: 0.600 - 0s 391us/step - loss: 0.6419 - acc: 0.5500\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6351 - acc: 0.600 - 0s 781us/step - loss: 0.6387 - acc: 0.5500\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6647 - acc: 0.500 - 0s 391us/step - loss: 0.6356 - acc: 0.5500\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6109 - acc: 0.600 - 0s 391us/step - loss: 0.6324 - acc: 0.5500\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7578 - acc: 0.300 - 0s 391us/step - loss: 0.6294 - acc: 0.5500\n",
      "Epoch 139/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5473 - acc: 0.700 - 0s 391us/step - loss: 0.6261 - acc: 0.5500\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6287 - acc: 0.500 - 0s 391us/step - loss: 0.6229 - acc: 0.5500\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6626 - acc: 0.500 - 0s 391us/step - loss: 0.6201 - acc: 0.5500\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6418 - acc: 0.500 - 0s 391us/step - loss: 0.6167 - acc: 0.5500\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6155 - acc: 0.600 - 0s 390us/step - loss: 0.6137 - acc: 0.5500\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5839 - acc: 0.600 - 0s 391us/step - loss: 0.6107 - acc: 0.5500\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5449 - acc: 0.700 - 0s 390us/step - loss: 0.6077 - acc: 0.5500\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5288 - acc: 0.700 - 0s 391us/step - loss: 0.6048 - acc: 0.5500\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6257 - acc: 0.500 - 0s 391us/step - loss: 0.6017 - acc: 0.5500\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5697 - acc: 0.600 - 0s 391us/step - loss: 0.5987 - acc: 0.5500\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6526 - acc: 0.400 - 0s 391us/step - loss: 0.5959 - acc: 0.5500\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6070 - acc: 0.500 - 0s 391us/step - loss: 0.5930 - acc: 0.5500\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6558 - acc: 0.400 - 0s 391us/step - loss: 0.5900 - acc: 0.5500\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5991 - acc: 0.500 - 0s 391us/step - loss: 0.5870 - acc: 0.5500\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5306 - acc: 0.700 - 0s 391us/step - loss: 0.5841 - acc: 0.5500\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6909 - acc: 0.300 - 0s 391us/step - loss: 0.5814 - acc: 0.5500\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5510 - acc: 0.600 - 0s 391us/step - loss: 0.5785 - acc: 0.5500\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5104 - acc: 0.700 - 0s 390us/step - loss: 0.5756 - acc: 0.5500\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6192 - acc: 0.500 - 0s 391us/step - loss: 0.5728 - acc: 0.5500\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.7145 - acc: 0.200 - 0s 391us/step - loss: 0.5703 - acc: 0.5500\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5473 - acc: 0.600 - 0s 391us/step - loss: 0.5673 - acc: 0.5500\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5404 - acc: 0.600 - 0s 390us/step - loss: 0.5645 - acc: 0.5500\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6552 - acc: 0.300 - 0s 391us/step - loss: 0.5618 - acc: 0.5500\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5360 - acc: 0.600 - 0s 391us/step - loss: 0.5591 - acc: 0.5500\n",
      "Epoch 163/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5410 - acc: 0.600 - 0s 390us/step - loss: 0.5563 - acc: 0.5500\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6583 - acc: 0.300 - 0s 391us/step - loss: 0.5539 - acc: 0.5500\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6006 - acc: 0.400 - 0s 390us/step - loss: 0.5510 - acc: 0.5500\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5239 - acc: 0.600 - 0s 391us/step - loss: 0.5485 - acc: 0.5500\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4962 - acc: 0.700 - 0s 390us/step - loss: 0.5457 - acc: 0.5500\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4297 - acc: 0.800 - 0s 391us/step - loss: 0.5432 - acc: 0.5500\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5230 - acc: 0.600 - 0s 390us/step - loss: 0.5405 - acc: 0.5500\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5515 - acc: 0.500 - 0s 390us/step - loss: 0.5380 - acc: 0.5500\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5031 - acc: 0.600 - 0s 391us/step - loss: 0.5354 - acc: 0.5500\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5649 - acc: 0.500 - 0s 391us/step - loss: 0.5330 - acc: 0.5500\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4034 - acc: 0.900 - 0s 390us/step - loss: 0.5304 - acc: 0.5500\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5893 - acc: 0.400 - 0s 391us/step - loss: 0.5279 - acc: 0.5750\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5579 - acc: 0.600 - 0s 391us/step - loss: 0.5254 - acc: 0.6500\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6287 - acc: 0.300 - 0s 391us/step - loss: 0.5231 - acc: 0.6500\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5008 - acc: 0.800 - 0s 390us/step - loss: 0.5204 - acc: 0.6500\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4941 - acc: 0.700 - 0s 390us/step - loss: 0.5180 - acc: 0.6500\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5004 - acc: 0.700 - 0s 455us/step - loss: 0.5155 - acc: 0.6500\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5042 - acc: 0.700 - 0s 391us/step - loss: 0.5131 - acc: 0.6750\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5375 - acc: 0.600 - 0s 390us/step - loss: 0.5107 - acc: 0.6750\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 0.5132 - acc: 0.600 - 0s 391us/step - loss: 0.5084 - acc: 0.7000\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5198 - acc: 0.700 - 0s 391us/step - loss: 0.5059 - acc: 0.7500\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4556 - acc: 0.800 - 0s 391us/step - loss: 0.5036 - acc: 0.7750\n",
      "Epoch 185/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4901 - acc: 0.800 - 0s 390us/step - loss: 0.5012 - acc: 0.8000\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5417 - acc: 0.800 - 0s 391us/step - loss: 0.4990 - acc: 0.8750\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5641 - acc: 0.900 - 0s 0us/step - loss: 0.4966 - acc: 0.8750\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4609 - acc: 0.800 - 0s 390us/step - loss: 0.4942 - acc: 0.9000\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4867 - acc: 0.800 - 0s 390us/step - loss: 0.4919 - acc: 0.9000\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4271 - acc: 0.900 - 0s 391us/step - loss: 0.4897 - acc: 0.9000\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5502 - acc: 0.800 - 0s 391us/step - loss: 0.4875 - acc: 0.9250\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4905 - acc: 0.900 - 0s 391us/step - loss: 0.4851 - acc: 0.9250\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4100 - acc: 0.900 - 0s 391us/step - loss: 0.4829 - acc: 0.9250\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.5522 - acc: 0.900 - 0s 391us/step - loss: 0.4809 - acc: 0.9250\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4513 - acc: 0.900 - 0s 390us/step - loss: 0.4785 - acc: 0.9250\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4670 - acc: 1.000 - 0s 391us/step - loss: 0.4762 - acc: 0.9250\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4033 - acc: 0.900 - 0s 391us/step - loss: 0.4741 - acc: 0.9250\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4298 - acc: 0.800 - 0s 391us/step - loss: 0.4720 - acc: 0.9250\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4846 - acc: 1.000 - 0s 391us/step - loss: 0.4697 - acc: 0.9250\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.4727 - acc: 0.900 - 0s 391us/step - loss: 0.4676 - acc: 0.9250\n",
      "60/60 [==============================] - ETA:  - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333174387614"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "np.random.seed(7)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,input_dim=4,activation='sigmoid'))\n",
    "\n",
    "    # Compile the model #\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    \n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=200, batch_size=10)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "scores = model.score(x_test, y_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'build_fn': <function __main__.create_model()>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #4\n",
    "\n",
    "#### UCI Adult salary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:33<00:00, 33.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "18088/18088 [==============================] - ETA: 10:30 - loss: 4.5332 - binary_crossentropy: 4.53 - ETA: 37s - loss: 3.7332 - binary_crossentropy: 3.7332 - ETA: 22s - loss: 3.8385 - binary_crossentropy: 3.83 - ETA: 14s - loss: 3.8105 - binary_crossentropy: 3.81 - ETA: 10s - loss: 3.8137 - binary_crossentropy: 3.81 - ETA: 7s - loss: 3.8221 - binary_crossentropy: 3.8221 - ETA: 6s - loss: 3.8760 - binary_crossentropy: 3.876 - ETA: 5s - loss: 3.8771 - binary_crossentropy: 3.877 - ETA: 4s - loss: 3.8629 - binary_crossentropy: 3.862 - ETA: 4s - loss: 3.8735 - binary_crossentropy: 3.873 - ETA: 3s - loss: 3.8697 - binary_crossentropy: 3.869 - ETA: 3s - loss: 3.8934 - binary_crossentropy: 3.893 - ETA: 3s - loss: 3.8981 - binary_crossentropy: 3.898 - ETA: 2s - loss: 3.9148 - binary_crossentropy: 3.914 - ETA: 2s - loss: 3.9225 - binary_crossentropy: 3.922 - ETA: 2s - loss: 3.9327 - binary_crossentropy: 3.932 - ETA: 2s - loss: 3.9087 - binary_crossentropy: 3.908 - ETA: 1s - loss: 3.9333 - binary_crossentropy: 3.933 - ETA: 1s - loss: 3.9370 - binary_crossentropy: 3.937 - ETA: 1s - loss: 3.9250 - binary_crossentropy: 3.925 - ETA: 1s - loss: 3.9088 - binary_crossentropy: 3.908 - ETA: 1s - loss: 3.9022 - binary_crossentropy: 3.902 - ETA: 1s - loss: 3.8959 - binary_crossentropy: 3.895 - ETA: 1s - loss: 3.9257 - binary_crossentropy: 3.925 - ETA: 0s - loss: 3.9213 - binary_crossentropy: 3.921 - ETA: 0s - loss: 3.9276 - binary_crossentropy: 3.927 - ETA: 0s - loss: 3.9346 - binary_crossentropy: 3.934 - ETA: 0s - loss: 3.9486 - binary_crossentropy: 3.948 - ETA: 0s - loss: 3.9456 - binary_crossentropy: 3.945 - ETA: 0s - loss: 3.9489 - binary_crossentropy: 3.948 - ETA: 0s - loss: 3.9473 - binary_crossentropy: 3.947 - ETA: 0s - loss: 3.9499 - binary_crossentropy: 3.949 - ETA: 0s - loss: 3.9469 - binary_crossentropy: 3.946 - ETA: 0s - loss: 3.9507 - binary_crossentropy: 3.950 - ETA: 0s - loss: 3.9612 - binary_crossentropy: 3.961 - 3s 160us/step - loss: 3.9609 - binary_crossentropy: 3.9609\n",
      "27134/27134 [==============================] - ETA: 27 - ETA: 1 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 38us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_adult_salary\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\" \", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)\n",
    "\n",
    "# Split the dataset into test and train datasets\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #5\n",
    "\n",
    "#### UCI Ad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "Epoch 1/1\n",
      "943/943 [==============================] - ETA: 6s - loss: 0.0594 - binary_crossentropy: 0.059 - ETA: 0s - loss: 0.1452 - binary_crossentropy: 0.145 - 0s 345us/step - loss: 0.1602 - binary_crossentropy: 0.1602\n",
      "1416/1416 [==============================] - ETA:  - ETA:  - 0s 61us/step\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad\"\n",
    "dataset_info = experiment_automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #6\n",
    "\n",
    "#### UCI Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LinearRegression'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "model_name = 'linear_regression'\n",
    "model_mappings = {\n",
    "    'linear_regression': 'LinearRegression',\n",
    "    'logistic_regression': 'LogisticRegression'\n",
    "}\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "for key, value in model_mappings.items():\n",
    "    if key == model_name:\n",
    "        name = value\n",
    "\n",
    "module = __import__('sklearn.linear_model', fromlist=[name])\n",
    "imported_module = getattr(module, name)\n",
    "model = imported_module\n",
    "\n",
    "primal_model = model()\n",
    "\n",
    "# Primal\n",
    "primal_model.fit(x_train, y_train)\n",
    "primal_model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data\n",
    "# sc = StandardScaler()\n",
    "# diabetes.data = sc.fit_transform(diabetes.data)\n",
    "\n",
    "X = diabetes.data\n",
    "Y = diabetes.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "# # Split the data into training/testing sets\n",
    "# x_train = diabetes_X[:-20]\n",
    "# x_test = diabetes_X[-20:]\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# y_train = diabetes.target[:-20]\n",
    "# y_test = diabetes.target[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = np.column_stack([X,Y])\n",
    "# np.savetxt(\"diabetes.csv\", temp_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shakk\\\\Anaconda2\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\winmltools\\\\__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import winmltools\n",
    "winmltools.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "\n",
    "def f1(**kwargs):\n",
    "    params_json = json.load(open('../imly/architectures/sklearn/params.json'))\n",
    "    params = params_json['params']\n",
    "    kwargs.setdefault('params', params)\n",
    "    kwargs.setdefault('x_train', np.array([[1], [2]]))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(kwargs['params']['first_neuron'],\n",
    "                    input_dim=kwargs['x_train'].shape[1],\n",
    "                    activation=kwargs['params']['activation']))\n",
    "\n",
    "    model.compile(optimizer=kwargs['params']['optimizer'],\n",
    "                  loss=kwargs['params']['losses'],\n",
    "                  metrics=['acc'])\n",
    "    onnx_model = onnxmltools.convert_keras(model, target_opset=8)\n",
    "    print(type(model))\n",
    "    onnx_model\n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.sequential.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onnx.onnx_ml_pb2.ModelProto"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)\n",
    "# cross check import (f1p1 and f2p2 combination) - Is it possible to edit after the export-import flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx.save(model, './onnx_model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test bed ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LogisticRegression': True}\n",
      "{'LinearRegression': True}\n"
     ]
    }
   ],
   "source": [
    "mapping = { \"KerasClassifier\": {\n",
    "    \"LogisticRegression\": True\n",
    "},\n",
    " \"KerasRegressor\": {\n",
    "     \"LinearRegression\": True\n",
    " }\n",
    "}\n",
    "\n",
    "name = \"LinearRegression\"\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    test = mapping[key]\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
